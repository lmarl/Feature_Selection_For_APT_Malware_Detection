##################################
# IMPORTS
##################################
import numpy as np
from numpy import *
import pandas as pd
from time import time
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from matplotlib.ticker import NullFormatter
from sklearn import manifold
from sklearn.manifold import TSNE
from sklearn.feature_selection import VarianceThreshold
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
from sklearn import datasets
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.externals import joblib 
from sklearn import tree
import sys
import numpy
from sklearn.externals.six import StringIO
import pydot
import graphviz

##################################
# PREPARATIVOS
##################################

###Leemos el dataset
input_file="../datasets/reducido_a_1941_features_without_NaNs_y_19457_rows.csv"
output_file="resultados/codo_variance.png"
dump_file="../tmp/dataset_1941.joblib.pkl"
num_bars=400

##########
#ARGUMENTS#
##########
def syntax_error():
        print "SYNTAX ERROR: Introduce un argumentos con los siguientes valores"
        print " 1 Grafico de barras"
        print " 2 Grafico de lineas"
        print
        print "Current input/output values are:"
        print "         Input File:"+input_file
        print "         Dump File:"+dump_file
        print "         Output File:"+output_file
        print "         Num values:"+str(num_bars)
        print
        print

argumentos=sys.argv
if len(argumentos)!=2:
    syntax_error()
    exit()

df = pd.read_csv(input_file, header = 0, sep=';', dtype={"MD5":object,"FIRST_SEEN":object,"SIZE":int,"NUM_PACKERS":float64,"PACKERS_BIN":float64,"PACKERS_BIN_DIST":float64,"MALWARE_TYPE":object,"NUM_IMPORTS":float64,"IMPORTS_BIN":float64,"IMPORTS_BINARY_DIST":float64,"HAS_OVERLAYS":int,"SUSPICIOUS_DLLS":float64,"SUSPICIOUS_DLLS_DIST":float64,"ANTIDEBUG_BINARY":float64,"ANTIDEBUG_BINARY_DIST":float64,"NUM_LANG":float64,"LANG_BINARY":float64,"LANG_BINARY_DIST":float64,"API_BINARY":float64,"API_BINARY_DIST":float64,"RESOURCE_NUM":int,"SERVICES_BINARY":float64,"SERVICES_BINARY_DIST":float64,"all_files_binary":float64,"all_files_binary_DIST":float64,"all_opened_files_binary":float64,"all_opened_files_binary_DIST":float64,"all_written_files_binary":float64,"all_written_files_binary_DIST":float64,"all_deleted_files_binary":float64,"all_deleted_files_binary_DIST":float64,"all_read_files_binary":float64,"all_read_files_binary_DIST":float64,"UDP_Countries":float64,"UDP_Countries_DIST":float64,"TCP_countries":float64,"TCP_countries_DIST":float64,"DNS_countries":float64,"DNS_countries_DIST":float64,"SSDEEP":int,"IMPHASH":int,"APT":int,"NUM_APT":int})

###Separamos los campos que no nos interesan y campos objetivo
isapt=df.APT
numapt=df.NUM_APT

###Quitamos las cabeceras de los campos objetivo
values2=isapt.values.astype(np.int64)
values3=numapt.values.astype(np.int64)

print df.shape

###Quitamos los campos que no nos interesan
del df['APT']
del df['NUM_APT']
#del df['DEVEL_GROUP']
#del df['DEVEL_COUNTRY']

numpy.set_printoptions(threshold=numpy.nan)

##################################
# Mormalization (0 to 1)
##################################
from sklearn import preprocessing
headers=list(df)
x = df.values #returns a numpy array
min_max_scaler = preprocessing.MinMaxScaler()
x_scaled = min_max_scaler.fit_transform(x)
df = pd.DataFrame(x_scaled)

##################################
# VARIANZA
##################################
print(" ")
print("====================================================")
print("VARIANZA")
print("====================================================")

#from sklearn.feature_selection import SelectPercentile
#chi_results=SelectKBest(chi2,'all').fit(imputado,values2)
#chi_results=SelectKBest(chi2,400).fit(X,values2)
#chi_results=SelectPercentile(chi2,percentile=10).fit(imputado,values2)
#New_X=SelectKBest(chi2,2).fit_transform(imputado,values2)
variance=df.apply(np.var,axis=0)
#print(variance)
headersArray=array(array(headers))
print("==Los scores de cada campo son==")
scores=np.column_stack((headersArray,variance))
mat_sort = scores[scores[:,1].astype(np.float).argsort()]
print (mat_sort)
lista = pd.DataFrame({'header': headersArray, 'variance': variance})
lista.sort_values(by=['variance'], inplace=True, ascending=False)

sorted_chis=lista['variance']
sorted_headers=lista['header']

short_sorted_chis=sorted_chis[0:num_bars]
short_sorted_headers=sorted_headers[0:num_bars]

plt.locator_params(axis='y', nbins=16)

plt.title("Feature variances")
if (argumentos[1]=="1"):
    ###plt.bar(range(X.shape[1]), importances[indices], color="r", yerr=std[indices], align="center")
    plt.bar(range(len(short_sorted_chis)), short_sorted_chis, color="b", align="center")
    ###plt.xticks(range(X.shape[1]), indices)
    plt.xticks(range(len(short_sorted_chis)), short_sorted_headers, rotation=80)

    ax = plt.gca()
    ax.tick_params(axis = 'x', which = 'major', labelsize = 6, width=1)
    ax.tick_params(axis = 'x', which = 'minor', labelsize = 6, width=1)

    iii=0
    for tick in ax.xaxis.get_major_ticks():
        if iii%2==0:
            tick.label.set_color('red')
        if iii%2==1:
            tick.label.set_color('black')
        iii+=1

    plt.xlim([0, len(short_sorted_chis)])
    output_file="resultados/codo_variance_bars_1941.png"

if (argumentos[1]=="2"):
    plt.title("Feature variances")
    plt.plot(range(len(short_sorted_chis)), short_sorted_chis, color="b")
    output_file="resultados/codo_variance_line_1941.png"

plt.show()
plt.savefig(output_file)



###plt.xticks(range(X.shape[1]), indices)
#plt.xticks(range(len(short_sorted_importances)), short_sorted_headers, rotation=80)

#ax = plt.gca()
#ax.tick_params(axis = 'x', which = 'major', labelsize = 5, width=2)
#ax.tick_params(axis = 'x', which = 'minor', labelsize = 5, width=2)

#iii=0
#for tick in ax.xaxis.get_major_ticks(): 
    #if iii%2==0:
        #tick.label.set_color('red')
    #if iii%2==1:
        #tick.label.set_color('black')
    #iii+=1

#plt.xlim([0, len(short_sorted_importances)])

#joblib.dump(mat_sort, "../tmp/scores_extratree_sindate.pkl", compress=9)
#print(mat_sort)



#importances = model.feature_importances_
#std = np.std([tree.feature_importances_ for tree in model.estimators_], axis=0)
#indices = np.argsort(importances)[::-1]

# Print the feature ranking
#print("Feature ranking:")

#for f in range(X.shape[1]):
    #print("%d. feature %d (%f)" % (f + 1, indices[f], importances[indices[f]]))

# Plot the feature importances of the forest
#plt.figure()
#plt.title("Feature importances")
#plt.bar(range(X.shape[1]), importances[indices],
#       color="r", yerr=std[indices], align="center")
#plt.xticks(range(X.shape[1]), indices)
#plt.xlim([-1, X.shape[1]])
#plt.show()


# PRINT DECISSION TREE
#dot_data = tree.export_graphviz(model, out_file=None)
#dot_data = tree.export_graphviz(model, out_file=None, max_depth=4, feature_names=headers, class_names=['Malware','APT'], filled=True, rounded=True, special_characters=True)
#graph = graphviz.Source(dot_data)
#graph.render("mi_dataset")

#export_graphviz(model, out_file=dot_data, max_depth=4, feature_names=headers, class_names=['Malware','APT'], filled=True, rounded=True, special_characters=True)
#graph = graphviz.Source(dot_data)
#graph.render("arbol_extraTree")

exit()


##################################
# DECISION TREE IMPORTANCES
##################################
print("=================================")
print("====Decission Tree classifier====")
print("=================================")
print("Usando el clasificador Decission Tree")
from sklearn import tree

modelDT=tree.DecisionTreeClassifier()
#model.fit(df,values2)
modelDT.fit(X,values2)
scores=np.column_stack((headersArray,modelDT.feature_importances_))

mat_sort = scores[scores[:,1].astype(np.float).argsort()]
joblib.dump(mat_sort, "../tmp/scores_decission_sindate.pkl", compress=9)
print(mat_sort)

####################################
# FINAL ACCOUNTING
####################################

all_scores=np.column_stack((headersArray,chi_results.scores_, fitt.get_support(),  model.feature_importances_, modelDT.feature_importances_))

print (all_scores)
variances=fitt.get_support()

for index in range(0,chi_results.scores_.shape[1]):
	n=0
	if chi_results.scores_[index]!=nan:
		n=n+1;

	if variances[index]>0.0:
		n=n+1;

	if model.feature_importances_ > 0.0:
		n=n+1;

	if modelDT.feature_importances_ > 0.0:
		n=n+1;

	final_values[index]=n;

all_scores_final=np.column_stack((all_scores, final_values))

print all_scores_final
