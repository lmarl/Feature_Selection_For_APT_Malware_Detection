##################################
# IMPORTS
##################################
import numpy as np
from numpy import *
import pandas as pd
from time import time
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from matplotlib.ticker import NullFormatter
from sklearn import manifold
from sklearn.manifold import TSNE
from sklearn.feature_selection import VarianceThreshold
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
from sklearn import datasets
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.externals import joblib 
from sklearn import tree
import sys
import numpy
from sklearn.externals.six import StringIO
import pydot
import graphviz

input_file="../datasets/reducido_a_1941_features_without_NaNs_y_19457_rows.csv"
output_file="resultados/codo_extra_tree.png"
dump_file="../tmp/dataset_1941.joblib.pkl"
num_bars=200

##########
#ARGUMENTS#
##########
def syntax_error():
        print "SYNTAX ERROR: Introduce un argumentos con los siguientes valores"
        print " 1 Grafico de barras"
        print " 2 Grafico de lineas"
        print
        print "Current input/output values are:"
        print "         Input File:"+input_file
        print "         Dump File:"+dump_file
        print "         Output File:"+output_file
        print "         Num values:"+str(num_bars)
        print
        print

argumentos=sys.argv
if len(argumentos)!=2:
    syntax_error()
    exit()

##################################
# PREPARATIVOS
##################################

###Leemos el dataset

df = pd.read_csv(input_file, header = 0, sep=';', dtype={"MD5":object,"FIRST_SEEN":object,"SIZE":int,"NUM_PACKERS":float64,"PACKERS_BIN":float64,"PACKERS_BIN_DIST":float64,"MALWARE_TYPE":object,"NUM_IMPORTS":float64,"IMPORTS_BIN":float64,"IMPORTS_BINARY_DIST":float64,"HAS_OVERLAYS":int,"SUSPICIOUS_DLLS":float64,"SUSPICIOUS_DLLS_DIST":float64,"ANTIDEBUG_BINARY":float64,"ANTIDEBUG_BINARY_DIST":float64,"NUM_LANG":float64,"LANG_BINARY":float64,"LANG_BINARY_DIST":float64,"API_BINARY":float64,"API_BINARY_DIST":float64,"RESOURCE_NUM":int,"SERVICES_BINARY":float64,"SERVICES_BINARY_DIST":float64,"all_files_binary":float64,"all_files_binary_DIST":float64,"all_opened_files_binary":float64,"all_opened_files_binary_DIST":float64,"all_written_files_binary":float64,"all_written_files_binary_DIST":float64,"all_deleted_files_binary":float64,"all_deleted_files_binary_DIST":float64,"all_read_files_binary":float64,"all_read_files_binary_DIST":float64,"UDP_Countries":float64,"UDP_Countries_DIST":float64,"TCP_countries":float64,"TCP_countries_DIST":float64,"DNS_countries":float64,"DNS_countries_DIST":float64,"SSDEEP":int,"IMPHASH":int,"APT":int,"NUM_APT":int})

###Separamos los campos que no nos interesan y campos objetivo
isapt=df.APT
numapt=df.NUM_APT

df2=df[df.APT==1]
df3=df2[df2.isTrojan==1]
df4=df2[df2.isWorm==1]
df5=df2[df2.isBackdoor==1]
df6=df2[df2.isRootkit==1]
df7=df2[df2.isSpyware==1]
df8=df2[df2.isTypeUnknown==1]
df9=df2[df2.isOtherType==1]
print df3.shape
print df4.shape
print df5.shape
print df6.shape
print df7.shape
print df8.shape
print df9.shape

exit()

###Quitamos las cabeceras de los campos objetivo
values2=isapt.values.astype(np.int64)
values3=numapt.values.astype(np.int64)

import datetime
def convert_to_year(date_in_some_format):
	datetime_object = datetime.datetime.strptime(date_in_some_format, '%d/%m/%Y %H:%M')
	totalmins=(datetime_object-datetime.datetime(1970,1,1)).total_seconds()/60
	return int(totalmins)


###Quitamos los campos que no nos interesan
del df['APT']
del df['NUM_APT']
#del df['DEVEL_GROUP']
#del df['DEVEL_COUNTRY']

numpy.set_printoptions(threshold=numpy.nan)
#
X=df
joblib.dump(X, dump_file)

##################################
# EXTRA TREE IMPORTANCES
##################################
print("=================================")
print("====Extra Tree importances=======")
print("=================================")
print("Usando el clasificador Extra Tree")

headers=list(df)
headersArray=array(array(headers))

#model=tree.ExtraTreeClassifier()
model=ExtraTreesClassifier(n_estimators=250)

#values2 es una columna de 0s y 1s sobre esa muestra es de un APT o no.
model.fit(X,values2)
importances=model.feature_importances_
indices = np.argsort(importances)[::-1]
###std = np.std([tree.feature_importances_ for tree in model.estimators_], axis=0)

lista = pd.DataFrame({'header': headersArray, 'importance': importances})
lista.sort_values(by=['importance'], inplace=True, ascending=False)

sorted_importances=lista['importance']
sorted_headers=lista['header']

short_sorted_importances=sorted_importances[0:num_bars]
short_sorted_headers=sorted_headers[0:num_bars]

scores=np.column_stack((headersArray,model.feature_importances_))
mat_sort = scores[scores[:,1].astype(np.float).argsort()]

plt.figure()
plt.title("ExtraTree Feature importances")

if (argumentos[1]=="1"):
    print(mat_sort)
    #list=mat_sort
    ###plt.bar(range(X.shape[1]), importances[indices], color="r", yerr=std[indices], align="center")
    plt.bar(range(len(short_sorted_importances)), short_sorted_importances, color="r", align="center")
    ###plt.xticks(range(X.shape[1]), indices)
    plt.xticks(range(len(short_sorted_importances)), short_sorted_headers, rotation=80)

    ax = plt.gca()
    ax.tick_params(axis = 'x', which = 'major', labelsize = 5, width=2)
    ax.tick_params(axis = 'x', which = 'minor', labelsize = 5, width=2)

    iii=0
    for tick in ax.xaxis.get_major_ticks(): 
        if iii%2==0:
            tick.label.set_color('red')
        if iii%2==1:
            tick.label.set_color('black')
        iii+=1

    plt.xlim([0, len(short_sorted_importances)])
    output_file="resultados/codo_extra_tree_bars_1941.png"

if (argumentos[1]=="2"):
    plt.plot(range(len(short_sorted_importances)), short_sorted_importances, color="r")
    output_file="resultados/codo_extra_tree_lines_1941.png"


plt.savefig(output_file, bbox_inches='tight')
plt.show()
#joblib.dump(mat_sort, "../tmp/scores_extratree_sindate.pkl", compress=9)
#print(mat_sort)

#importances = model.feature_importances_
#std = np.std([tree.feature_importances_ for tree in model.estimators_], axis=0)
#indices = np.argsort(importances)[::-1]

# Print the feature ranking
#print("Feature ranking:")

#for f in range(X.shape[1]):
    #print("%d. feature %d (%f)" % (f + 1, indices[f], importances[indices[f]]))

# Plot the feature importances of the forest
#plt.figure()
#plt.title("Feature importances")
#plt.bar(range(X.shape[1]), importances[indices],
#       color="r", yerr=std[indices], align="center")
#plt.xticks(range(X.shape[1]), indices)
#plt.xlim([-1, X.shape[1]])
#plt.show()


# PRINT DECISSION TREE
#dot_data = tree.export_graphviz(model, out_file=None)
#dot_data = tree.export_graphviz(model, out_file=None, max_depth=4, feature_names=headers, class_names=['Malware','APT'], filled=True, rounded=True, special_characters=True)
#graph = graphviz.Source(dot_data)
#graph.render("mi_dataset")

#export_graphviz(model, out_file=dot_data, max_depth=4, feature_names=headers, class_names=['Malware','APT'], filled=True, rounded=True, special_characters=True)
#graph = graphviz.Source(dot_data)
#graph.render("arbol_extraTree")

exit()

##################################
# FEATURE SELECTION
##################################
print("===================================================")
print("====Remove features with low variance (>0,9)=======")
print("===================================================")
headers=list(df)
headersArray=array(array(headers))

#for thres in [ 0.05, 0.1, 0.15, 0.2, 0.3, 0.4, 0.5 ]:	
	#print ("PARA THRES=", thres)
	#Sel=VarianceThreshold(threshold=thres)
	#fitt=Sel.fit(X)
	#scores=np.column_stack((headersArray,fitt.get_support()))
	#support=fitt.get_support()
	#New_X=Sel.fit_transform(X)
	#mat_sort = scores[scores[:,1].argsort()]
	#print (np.sum(support))
	#print (np.count_nonzero(support))

Sel=VarianceThreshold(threshold=.1)
fitt=Sel.fit(X)
New_X=Sel.fit_transform(X)
print df.shape
print X.shape


#print headersArray.shape
#print("Son estos:")

scores=np.column_stack((headersArray,fitt.get_support(), fitt.variances_))

#print(scores.argsort())

##MAT_SORT CONTIENE UNA LISTA DE 1956 CAMPOS EN LOS QUE UNOS 135 TIENEN EL VALOR A TRUE Y SON LOS QUE SE DEBEN MANTENER (Con threshold > 0.9)
mat_sort = scores[scores[:,1].argsort()]
print(mat_sort)
print(mat_sort.shape)
joblib.dump(mat_sort, "../tmp/scores_varianza_sindate.pkl", compress=9)

exit()

##################################
# CHI2
##################################
print(" ")
print("====================================================")
print("====Univariate Feature Selection (Chi2, k=10)=======")
print("====================================================")

from sklearn.feature_selection import SelectPercentile
#chi_results=SelectKBest(chi2,'all').fit(imputado,values2)
chi_results=SelectKBest(chi2,100).fit(imputado,values2)
#chi_results=SelectPercentile(chi2,percentile=10).fit(imputado,values2)
#New_X=SelectKBest(chi2,2).fit_transform(imputado,values2)

headers=list(df)
headersArray=array(array(headers))
print("==Los scores de cada campo son==")
print(chi_results.scores_)
#print(New_X.scores_)
scores=np.column_stack((headersArray,chi_results.scores_))
mat_sort = scores[scores[:,1].astype(np.float).argsort()]
print(mat_sort)
print(" ")
exit()

##################################
# DECISION TREE IMPORTANCES
##################################
print("=================================")
print("====Decission Tree classifier====")
print("=================================")
print("Usando el clasificador Decission Tree")
from sklearn import tree

modelDT=tree.DecisionTreeClassifier()
#model.fit(df,values2)
modelDT.fit(X,values2)
scores=np.column_stack((headersArray,modelDT.feature_importances_))

mat_sort = scores[scores[:,1].astype(np.float).argsort()]
joblib.dump(mat_sort, "../tmp/scores_decission_sindate.pkl", compress=9)
print(mat_sort)

####################################
# FINAL ACCOUNTING
####################################

all_scores=np.column_stack((headersArray,chi_results.scores_, fitt.get_support(),  model.feature_importances_, modelDT.feature_importances_))

print (all_scores)
variances=fitt.get_support()

for index in range(0,chi_results.scores_.shape[1]):
	n=0
	if chi_results.scores_[index]!=nan:
		n=n+1;

	if variances[index]>0.0:
		n=n+1;

	if model.feature_importances_ > 0.0:
		n=n+1;

	if modelDT.feature_importances_ > 0.0:
		n=n+1;

	final_values[index]=n;

all_scores_final=np.column_stack((all_scores, final_values))

print all_scores_final
