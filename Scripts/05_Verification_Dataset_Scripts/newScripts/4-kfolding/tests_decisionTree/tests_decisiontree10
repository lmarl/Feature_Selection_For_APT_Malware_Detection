[?2004h[?1049h[22;0;0t[?1h=[?2004h[1;38r[?12h[?12l[27m[23m[29m[m[H[2J[?25l[38;1H"09_Mar_11_17_45_kfolding_desdecero_with_several_methods_sindate_solo230_sin_gaussian_10fold_testado.py"<_17_45_kfolding_desdecero_with_several_methods_sindate_solo230_sin_gaussian_10fold_testado.py" 971L, 36098C[3;1H[31m###########################################
# Decission Tree Classifier
###########################################
from sklearn import tree
from sklearn.model_selection import cross_val_predict
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import cross_val_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import precision_score, recall_score
from sklearn.metrics import classification_report

print("Starting Decission Tree:") 
end = time.time()
print(end - start)
#Creamos el clasificador. El random_state value no deberia afectar

for depth_ in 5,10,15,20,25,30,35,40,45,50,None:
    print("############################DEPTH=##############################")
    print(depth_)
    print("############################DEPTH=##############################")
    tree_clf = tree.DecisionTreeClassifier(max_depth=depth_)

    #Y lo entrenamos con el training Set
    fitted_tree_clf=tree_clf.fit(X_train, Y_train_APT)
    print("Finishing Decission Tree:") 
    end = time.time()
    print(end - start)
    
    ##Calculamos (A mano) el porcentaje de aciertos. Lo hacemos con los valores del test set
    acierto=0
    total=0
    for i in range(17000,20529):
        total=total+1
        ej=df3[i]
        prediction=tree_clf.predict([ej])[m[38;120H255,44[8C25%[19;44H[?25h[?25l[38;1HType  :qa!  and press <Enter> to abandon all changes and exit Vim[38;66H[K[38;120H255,44[8C25%[19;44H[?25h[?25l[38;110H^[[19;44H[38;110H  [19;44H[38;110H^[[19;44H[38;110H  [19;44H[?25h[?25l[38;110H:[19;44H[38;1H[K[38;1H:[?2004h[?25h