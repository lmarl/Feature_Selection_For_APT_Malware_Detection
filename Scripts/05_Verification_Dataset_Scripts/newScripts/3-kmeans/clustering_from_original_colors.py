import numpy as np
from numpy import *
import pandas as pd
import sys
from time import time
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from matplotlib.ticker import NullFormatter
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn import manifold
from sklearn.manifold import TSNE
from sklearn.cluster import AffinityPropagation
from sklearn.cluster import MeanShift, estimate_bandwidth
from sklearn.cluster import DBSCAN
from sklearn import metrics
from sklearn.cluster import AgglomerativeClustering
from sklearn.neighbors import kneighbors_graph
from sklearn.externals import joblib

##########
#FICHEROS#
##########
input_file= "../datasets/csv_a_empezar_desde_cero_v2_sin_cols21y37_con52_y_49_46_y43_40_34_31_28_24_17_14_10_final_con_cabecera_quitamos_5_muestras_develgroup_country_new_headers.csv"
data_dir="../tmp"
tmp_file="../tmp/tsne_para_kmeans.joblib.pkl"
perpl=30


##################################
# PREPARATIVOS
##################################


        ###Leemos el dataset

df = pd.read_csv(input_file, header = 0, sep=';', dtype={"MD5":object,"FIRST_SEEN":object,"SIZE":int,"NUM_PACKERS":float64,"PACKERS_BIN":float64,"PACKERS_BIN_DIST":float64,"MALWARE_TYPE":object,"NUM_IMPORTS":float64,"IMPORTS_BIN":float64,"IMPORTS_BINARY_DIST":float64,"HAS_OVERLAYS":int,"SUSPICIOUS_DLLS":float64,"SUSPICIOUS_DLLS_DIST":float64,"ANTIDEBUG_BINARY":float64,"ANTIDEBUG_BINARY_DIST":float64,"NUM_LANG":float64,"LANG_BINARY":float64,"LANG_BINARY_DIST":float64,"API_BINARY":float64,"API_BINARY_DIST":float64,"RESOURCE_NUM":int,"SERVICES_BINARY":float64,"SERVICES_BINARY_DIST":float64,"all_files_binary":float64,"all_files_binary_DIST":float64,"all_opened_files_binary":float64,"all_opened_files_binary_DIST":float64,"all_written_files_binary":float64,"all_written_files_binary_DIST":float64,"all_deleted_files_binary":float64,"all_deleted_files_binary_DIST":float64,"all_read_files_binary":float64,"all_read_files_binary_DIST":float64,"UDP_Countries":float64,"UDP_Countries_DIST":float64,"TCP_countries":float64,"TCP_countries_DIST":float64,"DNS_countries":float64,"DNS_countries_DIST":float64,"SSDEEP":int,"IMPHASH":int,"APT":int,"NUM_APT":int})

###Separamos los campos que no nos interesan y campos objetivo
malwaretypes=df.MALWARE_TYPE
ssdeeptypes=df.SSDEEP
imphashtypes=df.IMPHASH
apttypes=df.APT
numapt=df.NUM_APT
md5=df.MD5

###Quitamos las cabeceras de los campos objetivo
values=malwaretypes.values.astype(np.int64)
values2=apttypes.values.astype(np.int64)
values3=numapt.values.astype(np.int64)


###Quitamos los campos que no nos interesan
#del df['MALWARE_TYPE']
del df['APT']
del df['MD5']
#del df['FIRST_SEEN']
del df['SSDEEP']
del df['IMPHASH']
del df['NUM_APT']

import datetime
def convert_to_year(date_in_some_format):
        datetime_object = datetime.datetime.strptime(date_in_some_format, '%d/%m/%Y %H:%M')
        totalmins=(datetime_object-datetime.datetime(1970,1,1)).total_seconds()/60
        return int(totalmins)

df['DATE'] = df['FIRST_SEEN'].apply(convert_to_year)
#print (df['Date'])

del df['FIRST_SEEN']
del df['DATE']

####Quitamos los campos DIST


del df['UNKNOWN1']
del df['UNKNOWN2']
del df['UNKNOWN3']
del df['UNKNOWN4']
del df['UNKNOWN5']
del df['UNKNOWN6']
del df['UNKNOWN7']
del df['UNKNOWN8']
del df['UNKNOWN9']
del df['UNKNOWN10']
del df['UNKNOWN11']
del df['UNKNOWN12']
del df['UNKNOWN13']
del df['UNKNOWN15']
del df['UNKNOWN16']


del df['PACKERS_BIN_DIST']
del df['IMPORTS_BINARY_DIST']
del df['ANTIDEBUG_BINARY_DIST']
del df['LANG_BINARY_DIST']
del df['API_BINARY_DIST']
del df['SERVICES_BINARY_DIST']
del df['all_files_binary_DIST']
del df['all_opened_files_binary_DIST']
del df['all_written_files_binary_DIST']
del df['all_deleted_files_binary_DIST']
del df['all_read_files_binary_DIST']
del df['UDP_Countries_DIST']
del df['TCP_countries_DIST']
del df['DNS_countries_DIST']
del df['SUSPICIOUS_DLLS_DIST']
del df['PACKERS_BIN']
del df['IMPORTS_BIN']
del df['ANTIDEBUG_BINARY']
del df['LANG_BINARY']
del df['API_BINARY']
del df['SERVICES_BINARY']
del df['all_files_binary']
del df['all_opened_files_binary']
del df['all_written_files_binary']
del df['all_deleted_files_binary']
del df['all_read_files_binary']
del df['UDP_Countries']
del df['TCP_countries']
del df['DNS_countries']
del df['SUSPICIOUS_DLLS']
#

##################################
# IMPUTAMOS VALORES VACIOS
##################################
print ("Imputing empty fields...")
#X=joblib.load("../tmp/dataset_mas_completo_sin_imphash_sin_ssdeeP.joblib.pkl",  mmap_mode='r')

from sklearn.impute import SimpleImputer
imp = SimpleImputer(strategy="most_frequent")
X=imp.fit_transform(df)

##################################
# Standarization
##################################
print ("Standarization process...")
from sklearn import preprocessing
#df = preprocessing.scale(df)
X = preprocessing.scale(X)



print ("Starting clustering...")
##########################
##########################

###Creamos la figura
fig = plt.figure(figsize=(15, 15))
num=4

##############MEANS###########################3
#clustering = KMeans(n_clusters=num, random_state=0).fit(X)
#filename="kmeans_clustering"

#########AFFINITY PROPAGATION#####################################################
#clustering = AffinityPropagation(preference=-50).fit(reduced_data)
#filename="affinity_clustering"

#########MEAN SHIFT#####################################################
#bandwidth = estimate_bandwidth(Y, quantile=0.9, n_samples=500)
#clustering = MeanShift(bandwidth=bandwidth, bin_seeding=True)
#clustering.fit(reduced_data)
#filename="mean_shift"

#########DBSCAN#####################################################
#clustering = DBSCAN(eps=4, min_samples=50).fit(X)
#core_samples_mask = np.zeros_like(clustering.labels_, dtype=bool)
#core_samples_mask[clustering.core_sample_indices_] = True
#labels = clustering.labels_
#
## Number of clusters in labels, ignoring noise if present.
#n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)
#n_noise_ = list(labels).count(-1)
#
#print('Estimated number of clusters: %d' % n_clusters_)
#print('Estimated number of noise points: %d' % n_noise_)
#print("Silhouette Coefficient: %0.3f" % metrics.silhouette_score(X, labels))
#filename="dbscan"

#########AGGLOMERATIVE CLUSTERING#####################################################
from sklearn.cluster import AgglomerativeClustering
clustering = AgglomerativeClustering(linkage='complete', n_clusters=4)
clustering.fit(X)
filename="HCA"


print ("Dimension reduction...")
########TSNE#####################################################
#tsne = manifold.TSNE(n_components=3, perplexity=30)
#Y = tsne.fit_transform(X)
#joblib.dump(Y,tmp_file)
Y=joblib.load(tmp_file,  mmap_mode='r')

print ("Plotting...")
###Pintamos los clusters
ax = fig.add_subplot(111, projection='3d')
ax.scatter(Y[:, 0], Y[:, 1], Y[:,2], cmap=plt.cm.Spectral, s=2, c=clustering.labels_.astype(float))

###Y pintamos los centroides
#centroids = clustering.cluster_centers_
#ax.scatter(centroids[:, 0], centroids[:, 1], centroids[:,2],marker='x', s=169, linewidths=3, color='black', zorder=10)
#plt.title("CLustering with %s groups" % (num) );
#plt.title('Estimated number of clusters: %d' % n_clusters_)


##################################
# SHOW PLOT
##################################
#ax.xaxis.set_major_formatter(NullFormatter())
#ax.yaxis.set_major_formatter(NullFormatter())
plt.axis('tight')
fig.savefig('resultados/HCA.png')
plt.show()
