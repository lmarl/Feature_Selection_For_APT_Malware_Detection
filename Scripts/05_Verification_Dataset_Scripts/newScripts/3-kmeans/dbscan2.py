import numpy as np
from numpy import *
from time import time
import pandas as pd
import sys
from time import time
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from matplotlib.ticker import NullFormatter
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn import manifold
from sklearn.manifold import TSNE
from sklearn.cluster import AffinityPropagation
from sklearn.cluster import MeanShift, estimate_bandwidth
from sklearn.cluster import DBSCAN
from sklearn import metrics
from sklearn.cluster import AgglomerativeClustering
from sklearn.neighbors import kneighbors_graph
import time

##########
#FICHEROS#
##########
input_file= "../datasets/reducido_a_1941_features_without_NaNs_y_19457_rows.csv"

data_dir="../tmp"
data_filename = data_dir + '/tsne_post_dbscan_3_clusters.npy'
tmp_file="../tmp/dataset_limpiado_imputado_standarizado.joblib.pkl"
output_file='resultados/dbscan_3clusters.png'

#from sklearn.datasets.samples_generator import make_blobs
#centers = [[1, 1], [-1, -1], [1, -1]]
#X, labels_true = make_blobs(n_samples=750, centers=centers, cluster_std=0.4, random_state=0)
#print labels_true
#print labels_true.shape

##########
#ARGUMENTS#
##########
print "Current input/output values are:"
print "         Input File:"+input_file
print "         TMP File:"+tmp_file
print "         Output File:"+output_file
print

start = time.time()
##################################
# PREPARATIVOS
##################################

###Leemos el dataset
df = pd.read_csv(input_file, header = 0, sep=';', dtype={"MD5":object,"FIRST_SEEN":object,"SIZE":int,"NUM_PACKERS":float64,"PACKERS_BIN":float64,"PACKERS_BIN_DIST":float64,"MALWARE_TYPE":object,"NUM_IMPORTS":float64,"IMPORTS_BIN":float64,"IMPORTS_BINARY_DIST":float64,"HAS_OVERLAYS":int,"SUSPICIOUS_DLLS":float64,"SUSPICIOUS_DLLS_DIST":float64,"ANTIDEBUG_BINARY":float64,"ANTIDEBUG_BINARY_DIST":float64,"NUM_LANG":float64,"LANG_BINARY":float64,"LANG_BINARY_DIST":float64,"API_BINARY":float64,"API_BINARY_DIST":float64,"RESOURCE_NUM":int,"SERVICES_BINARY":float64,"SERVICES_BINARY_DIST":float64,"all_files_binary":float64,"all_files_binary_DIST":float64,"all_opened_files_binary":float64,"all_opened_files_binary_DIST":float64,"all_written_files_binary":float64,"all_written_files_binary_DIST":float64,"all_deleted_files_binary":float64,"all_deleted_files_binary_DIST":float64,"all_read_files_binary":float64,"all_read_files_binary_DIST":float64,"UDP_Countries":float64,"UDP_Countries_DIST":float64,"TCP_countries":float64,"TCP_countries_DIST":float64,"DNS_countries":float64,"DNS_countries_DIST":float64,"SSDEEP":int,"IMPHASH":int,"APT":int,"NUM_APT":int})

###Separamos los campos que no nos interesan y campos objetivo
#malwaretypes=df.MALWARE_TYPE
#ssdeeptypes=df.SSDEEP
#imphashtypes=df.IMPHASH
apttypes=df.APT
numapt=df.NUM_APT
#md5=df.MD5

###Quitamos las cabeceras de los campos objetivo
#values=malwaretypes.values.astype(np.int64)
isAPT=apttypes.values.astype(np.int64)
values3=numapt.values.astype(np.int64)

#print isAPT
#print isAPT.shape

##################################
# Standarization
##################################
print ("Standarization process...")
end = time.time()
print(end - start)
start = time.time()
from sklearn import preprocessing
X = preprocessing.scale(df)



##################################
# DBSCAN
##################################
print ("DBSCAN process...")
end = time.time()
print(end - start)
start = time.time()
###Creamos la figura
fig = plt.figure(figsize=(15, 15))
n_clusters_=4
####reduced_data=Y
reduced_data=X

###Calculamos el KNN:
#kmeans = KMeans(n_clusters=n_clusters_, random_state=0).fit(reduced_data)
dbscan = DBSCAN(eps=0.2, min_samples=200).fit(reduced_data)

#df3=pd.DataFrame(data=reduced_data[0:,0:], index=range(0,reduced_data[1:,0], columns=reduced_data[0,1:])
df3=pd.DataFrame(data=reduced_data)
#pd.DataFrame(data=data[1:,1:],    # values
#...              index=data[1:,0],    # 1st column as index
#...              columns=data[0,1:])

##############################################################
#kmeans = AffinityPropagation(preference=-50).fit(reduced_data)

##############################################################
#bandwidth = estimate_bandwidth(reduced_data, quantile=0.9, n_samples=500)
#kmeans = MeanShift(bandwidth=bandwidth, bin_seeding=True)
#kmeans.fit(reduced_data)

print ("Dbscan predicting...")
end = time.time()
print(end - start)
start = time.time()
#predict=dbscan.predict(reduced_data)
predict= dbscan.fit_predict(reduced_data)
#print predict
df3['estimated_cluster']=pd.Series(predict, index=df3.index)
values4=pd.Series(predict, index=df3.index)

#print (values4)
#exit()

##############################################################
###Calculamos el DBSCAN:
#kmeans = DBSCAN(eps=4, min_samples=200).fit(reduced_data)
#core_samples_mask = np.zeros_like(kmeans.labels_, dtype=bool)
#core_samples_mask[kmeans.core_sample_indices_] = True
#labels = kmeans.labels_
#labels_true=isAPT		
#
## Number of clusters in labels, ignoring noise if present.
#n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)
#n_noise_ = list(labels).count(-1)
#
#print('Estimated number of clusters: %d' % n_clusters_)
#print('Estimated number of noise points: %d' % n_noise_)
#print("Homogeneity: %0.3f" % metrics.homogeneity_score(labels_true, labels))
#print("Completeness: %0.3f" % metrics.completeness_score(labels_true, labels))
#print("V-measure: %0.3f" % metrics.v_measure_score(labels_true, labels))
#print("Adjusted Rand Index: %0.3f" % metrics.adjusted_rand_score(labels_true, labels))
#print("Adjusted Mutual Information: %0.3f" % metrics.adjusted_mutual_info_score(labels_true, labels))
#print("Silhouette Coefficient: %0.3f" % metrics.silhouette_score(reduced_data, labels))

##############################################################
#kmeans = AgglomerativeClustering(linkage='average', connectivity=None, n_clusters=2)
#kmeans.fit(reduced_data)

#for eps_ in [0.2, 0.3, 0.6, 1,2, 4, 8, 12, 24, 48, 96]:
#	for min_samples_ in [25, 50, 75, 100, 200, 400, 600]:
#		db=DBSCAN(eps=eps_, min_samples=min_samples_). fit(reduced_data)
#		core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
#		core_samples_mask[db.core_sample_indices_] = True
#		labels = db.labels_
#		n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)
#		n_noise_ = list(labels).count(-1)
#
#		print ("PARA EPS=%f y MIN_SIZE=%d, salen %d clusters:" , eps_, min_samples_, n_clusters_)
#		
#		if (n_clusters_ > 0):
#			print('Estimated number of clusters: %d' % n_clusters_)
#			print('Estimated number of noise points: %d' % n_noise_)
#			print("Homogeneity: %0.3f" % metrics.homogeneity_score(labels_true, labels))
#			print("Completeness: %0.3f" % metrics.completeness_score(labels_true, labels))
#			print("V-measure: %0.3f" % metrics.v_measure_score(labels_true, labels))
#			print("Adjusted Rand Index: %0.3f" % metrics.adjusted_rand_score(labels_true, labels))


print ("Calculando TSNE...")
end = time.time()
print(end - start)
start = time.time()

Y = np.load(data_filename)

#tsne = manifold.TSNE(n_components=3, perplexity=30)
#Y = tsne.fit_transform(reduced_data)
#np.save(data_filename, Y)

print ("Pintando...")
end = time.time()
print(end - start)
start = time.time()
###Pintamos los clusters
ax = fig.add_subplot(111, projection='3d')

#ax.scatter(reduced_data[:, 0], reduced_data[:, 1], reduced_data[:,2], cmap=plt.cm.Spectral, s=2, c=kmeans.labels_.astype(float))
ax.scatter(Y[:, 0], Y[:, 1], Y[:,2], cmap=plt.cm.Spectral, s=2, c=values4)

###Y pintamos los centroides
#centroids = kmeans.cluster_centers_
#ax.scatter(centroids[:, 0], centroids[:, 1], centroids[:,2],marker='x', s=169, linewidths=3, color='black', zorder=10)
#plt.title("Kmeans with %s groups" % (num+1) );
plt.title('Estimated number of clusters: %d' % n_clusters_)


##################################
# SHOW PLOT
##################################
#ax.xaxis.set_major_formatter(NullFormatter())
#ax.yaxis.set_major_formatter(NullFormatter())
plt.axis('tight')
fig.savefig(output_file)
plt.show()
