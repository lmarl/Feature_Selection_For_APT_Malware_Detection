import numpy as np
from numpy import *
import pandas as pd
import sys
from time import time
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from matplotlib.ticker import NullFormatter
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn import manifold
from sklearn.manifold import TSNE
from sklearn.cluster import AffinityPropagation
from sklearn.cluster import MeanShift, estimate_bandwidth
from sklearn.cluster import DBSCAN
from sklearn import metrics
from sklearn.cluster import AgglomerativeClustering
from sklearn.neighbors import kneighbors_graph

##########
#FICHEROS#
##########
input_file= "../datasets/csv_a_empezar_desde_cero_v2_sin_cols21y37_con52_y_49_46_y43_40_34_31_28_24_17_14_10_final_con_cabecera_quitamos_5_muestras_develgroup_country_new_headers.csv"
data_dir="../tmp"
data_filename = data_dir + '/tsne_nuevo_desdecero_3D_sindate.npy'
tmp_file="../tmp/dataset_limpiado_imputado_standarizado.joblib.pkl"
output_file='resultados/kmeans_no_TSNE.png'

#from sklearn.datasets.samples_generator import make_blobs
#centers = [[1, 1], [-1, -1], [1, -1]]
#X, labels_true = make_blobs(n_samples=750, centers=centers, cluster_std=0.4, random_state=0)
#print labels_true
#print labels_true.shape

##########
#ARGUMENTS#
##########
def syntax_error():
        print "SYNTAX ERROR: Introduce un argumentos con los siguientes valores"
        print " 1 Recalcular TSNE"
        print " 2 Obtener calculo de la ultima operacion grabada"
        print
        print "Current input/output values are:"
        print "         Input File:"+input_file
        print "         TMP File:"+tmp_file
        print "         TSNE File:"+data_filename
        print "         Output File:"+output_file
        print
        print


argumentos=sys.argv
if len(argumentos)!=2:
        syntax_error()
        sys.exit()
else:
        if (argumentos[1]!='1' and argumentos[1]!='2'):
                syntax_error()
                sys.exit()

##################################
# PREPARATIVOS
##################################


        ###Leemos el dataset

df = pd.read_csv(input_file, header = 0, sep=';', dtype={"MD5":object,"FIRST_SEEN":object,"SIZE":int,"NUM_PACKERS":float64,"PACKERS_BIN":float64,"PACKERS_BIN_DIST":float64,"MALWARE_TYPE":object,"NUM_IMPORTS":float64,"IMPORTS_BIN":float64,"IMPORTS_BINARY_DIST":float64,"HAS_OVERLAYS":int,"SUSPICIOUS_DLLS":float64,"SUSPICIOUS_DLLS_DIST":float64,"ANTIDEBUG_BINARY":float64,"ANTIDEBUG_BINARY_DIST":float64,"NUM_LANG":float64,"LANG_BINARY":float64,"LANG_BINARY_DIST":float64,"API_BINARY":float64,"API_BINARY_DIST":float64,"RESOURCE_NUM":int,"SERVICES_BINARY":float64,"SERVICES_BINARY_DIST":float64,"all_files_binary":float64,"all_files_binary_DIST":float64,"all_opened_files_binary":float64,"all_opened_files_binary_DIST":float64,"all_written_files_binary":float64,"all_written_files_binary_DIST":float64,"all_deleted_files_binary":float64,"all_deleted_files_binary_DIST":float64,"all_read_files_binary":float64,"all_read_files_binary_DIST":float64,"UDP_Countries":float64,"UDP_Countries_DIST":float64,"TCP_countries":float64,"TCP_countries_DIST":float64,"DNS_countries":float64,"DNS_countries_DIST":float64,"SSDEEP":int,"IMPHASH":int,"APT":int,"NUM_APT":int})

###Separamos los campos que no nos interesan y campos objetivo
malwaretypes=df.MALWARE_TYPE
ssdeeptypes=df.SSDEEP
imphashtypes=df.IMPHASH
apttypes=df.APT
numapt=df.NUM_APT
md5=df.MD5

###Quitamos las cabeceras de los campos objetivo
values=malwaretypes.values.astype(np.int64)
isAPT=apttypes.values.astype(np.int64)
values3=numapt.values.astype(np.int64)

print isAPT
print isAPT.shape

if argumentos[1]=='2':
        ###O LO ABRIMOS
        print ("Reading NPY file...")
        Y = np.load(data_filename)
        X=Y
else:
        ###Quitamos los campos que no nos interesan
        #del df['MALWARE_TYPE']
        del df['APT']
        del df['MD5']
        #del df['FIRST_SEEN']
        del df['SSDEEP']
        del df['IMPHASH']
        del df['NUM_APT']

        import datetime
        def convert_to_year(date_in_some_format):
                datetime_object = datetime.datetime.strptime(date_in_some_format, '%d/%m/%Y %H:%M')
                totalmins=(datetime_object-datetime.datetime(1970,1,1)).total_seconds()/60
                return int(totalmins)

        df['DATE'] = df['FIRST_SEEN'].apply(convert_to_year)
        #print (df['Date'])

        del df['FIRST_SEEN']
        del df['DATE']

        ####Quitamos los campos DIST


        del df['UNKNOWN1']
        del df['UNKNOWN2']
        del df['UNKNOWN3']
        del df['UNKNOWN4']
        del df['UNKNOWN5']
        del df['UNKNOWN6']
        del df['UNKNOWN7']
        del df['UNKNOWN8']
        del df['UNKNOWN9']
        del df['UNKNOWN10']
        del df['UNKNOWN11']
        del df['UNKNOWN12']
        del df['UNKNOWN13']
        del df['UNKNOWN15']
        del df['UNKNOWN16']


        del df['PACKERS_BIN_DIST']
        del df['IMPORTS_BINARY_DIST']
        del df['ANTIDEBUG_BINARY_DIST']
        del df['LANG_BINARY_DIST']
        del df['API_BINARY_DIST']
        del df['SERVICES_BINARY_DIST']
        del df['all_files_binary_DIST']
        del df['all_opened_files_binary_DIST']
        del df['all_written_files_binary_DIST']
        del df['all_deleted_files_binary_DIST']
        del df['all_read_files_binary_DIST']
        del df['UDP_Countries_DIST']
        del df['TCP_countries_DIST']
        del df['DNS_countries_DIST']
        del df['SUSPICIOUS_DLLS_DIST']
        del df['PACKERS_BIN']
        del df['IMPORTS_BIN']
        del df['ANTIDEBUG_BINARY']
        del df['LANG_BINARY']
        del df['API_BINARY']
        del df['SERVICES_BINARY']
        del df['all_files_binary']
        del df['all_opened_files_binary']
        del df['all_written_files_binary']
        del df['all_deleted_files_binary']
        del df['all_read_files_binary']
        del df['UDP_Countries']
        del df['TCP_countries']
        del df['DNS_countries']
        del df['SUSPICIOUS_DLLS']

        print df.shape
        #

        ##################################
        # IMPUTAMOS VALORES VACIOS
        ##################################
        print ("Imputing empty fields...")
        #X=joblib.load("../tmp/dataset_mas_completo_sin_imphash_sin_ssdeeP.joblib.pkl",  mmap_mode='r')

        from sklearn.impute import SimpleImputer
        imp = SimpleImputer(strategy="most_frequent")
        X=imp.fit_transform(df)

        ##################################
        # Standarization
        ##################################
        print ("Standarization process...")
        from sklearn import preprocessing
        X = preprocessing.scale(X)

        joblib.dump(X, tmp_file)

        ##################################
        # TSNE
        ##################################

        #if argumentos[1]=='1':
        #        ###CALCULAMOS EL TSNE
        #        tsne = manifold.TSNE(n_components=3, perplexity=perpl)
        #        Y = tsne.fit_transform(X)
        #        np.save(data_filename, Y)

#if argumentos[1]=='2':
        ####O LO ABRIMOS
        #print ("Reading NPY file...")
        ##Y = np.load(data_filename)

##################################
# KMEANS
##################################

###Creamos la figura
fig = plt.figure(figsize=(15, 15))
num=1
####reduced_data=Y
reduced_data=X

###Calculamos el KNN:
kmeans = KMeans(n_clusters=num+1, random_state=0).fit(reduced_data)


##############################################################
#kmeans = AffinityPropagation(preference=-50).fit(reduced_data)

##############################################################
bandwidth = estimate_bandwidth(Y, quantile=0.9, n_samples=500)
kmeans = MeanShift(bandwidth=bandwidth, bin_seeding=True)
kmeans.fit(reduced_data)

##############################################################
###Calculamos el DBSCAN:
kmeans = DBSCAN(eps=4, min_samples=200).fit(reduced_data)
core_samples_mask = np.zeros_like(kmeans.labels_, dtype=bool)
core_samples_mask[kmeans.core_sample_indices_] = True
labels = kmeans.labels_
labels_true=isAPT		

# Number of clusters in labels, ignoring noise if present.
n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)
n_noise_ = list(labels).count(-1)

print('Estimated number of clusters: %d' % n_clusters_)
print('Estimated number of noise points: %d' % n_noise_)
print("Homogeneity: %0.3f" % metrics.homogeneity_score(labels_true, labels))
print("Completeness: %0.3f" % metrics.completeness_score(labels_true, labels))
print("V-measure: %0.3f" % metrics.v_measure_score(labels_true, labels))
print("Adjusted Rand Index: %0.3f" % metrics.adjusted_rand_score(labels_true, labels))
print("Adjusted Mutual Information: %0.3f" % metrics.adjusted_mutual_info_score(labels_true, labels))
#print("Silhouette Coefficient: %0.3f" % metrics.silhouette_score(reduced_data, labels))

##############################################################
kmeans = AgglomerativeClustering(linkage='average', connectivity=None, n_clusters=2)
kmeans.fit(reduced_data)

for eps_ in [0.2, 0.3, 0.6, 1,2, 4, 8, 12, 24, 48, 96]:
	for min_samples_ in [25, 50, 75, 100, 200, 400, 600]:
		db=DBSCAN(eps=eps_, min_samples=min_samples_). fit(reduced_data)
		core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
		core_samples_mask[db.core_sample_indices_] = True
		labels = db.labels_
		n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)
		n_noise_ = list(labels).count(-1)


		print ("PARA EPS=%f y MIN_SIZE=%d, salen %d clusters:" , eps_, min_samples_, n_clusters_)
		
		if (n_clusters_ > 0):
			print('Estimated number of clusters: %d' % n_clusters_)
			print('Estimated number of noise points: %d' % n_noise_)
			print("Homogeneity: %0.3f" % metrics.homogeneity_score(labels_true, labels))
			print("Completeness: %0.3f" % metrics.completeness_score(labels_true, labels))
			print("V-measure: %0.3f" % metrics.v_measure_score(labels_true, labels))
			print("Adjusted Rand Index: %0.3f" % metrics.adjusted_rand_score(labels_true, labels))
	
tsne = manifold.TSNE(n_components=3, perplexity=30)
Y = tsne.fit_transform(X)
np.save(data_filename, Y)

###Pintamos los clusters
ax = fig.add_subplot(111, projection='3d')

ax.scatter(Y[:, 0], Y[:, 1], Y[:,2], cmap=plt.cm.Spectral, s=2, c=kmeans.labels_.astype(float))

###Y pintamos los centroides
#centroids = kmeans.cluster_centers_
#ax.scatter(centroids[:, 0], centroids[:, 1], centroids[:,2],marker='x', s=169, linewidths=3, color='black', zorder=10)
#plt.title("Kmeans with %s groups" % (num+1) );
plt.title('Estimated number of clusters: %d' % n_clusters_)


##################################
# SHOW PLOT
##################################
#ax.xaxis.set_major_formatter(NullFormatter())
#ax.yaxis.set_major_formatter(NullFormatter())
plt.axis('tight')
fig.savefig('resultados/kmeans_2_groups.png')
plt.show()
