##################################
# IMPORTS
##################################
import numpy as np
from numpy import *
import pandas as pd
from time import time
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from matplotlib.ticker import NullFormatter
from sklearn import manifold
from sklearn.manifold import TSNE
from sklearn.feature_selection import VarianceThreshold
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
from sklearn import datasets
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.externals import joblib 
from sklearn import tree
import sys
import numpy
from sklearn.externals.six import StringIO
import pydot
import graphviz
##################################
# PREPARATIVOS
##################################

###Leemos el dataset
input_file="../datasets/csv_a_empezar_desde_cero_v2_sin_cols21y37_con52_y_49_46_y43_40_34_31_28_24_17_14_10_final_con_cabecera_quitamos_5_muestras_develgroup_country_new_headers.csv"

##Necesitamos leer el CSV original porque de aqui sacamos los campos "Y" (objetivos)
df = pd.read_csv(input_file, header = 0, sep=';', dtype={"MD5":object,"FIRST_SEEN":object,"SIZE":int,"NUM_PACKERS":float64,"PACKERS_BIN":float64,"PACKERS_BIN_DIST":float64,"MALWARE_TYPE":object,"NUM_IMPORTS":float64,"IMPORTS_BIN":float64,"IMPORTS_BINARY_DIST":float64,"HAS_OVERLAYS":int,"SUSPICIOUS_DLLS":float64,"SUSPICIOUS_DLLS_DIST":float64,"ANTIDEBUG_BINARY":float64,"ANTIDEBUG_BINARY_DIST":float64,"NUM_LANG":float64,"LANG_BINARY":float64,"LANG_BINARY_DIST":float64,"API_BINARY":float64,"API_BINARY_DIST":float64,"RESOURCE_NUM":int,"SERVICES_BINARY":float64,"SERVICES_BINARY_DIST":float64,"all_files_binary":float64,"all_files_binary_DIST":float64,"all_opened_files_binary":float64,"all_opened_files_binary_DIST":float64,"all_written_files_binary":float64,"all_written_files_binary_DIST":float64,"all_deleted_files_binary":float64,"all_deleted_files_binary_DIST":float64,"all_read_files_binary":float64,"all_read_files_binary_DIST":float64,"UDP_Countries":float64,"UDP_Countries_DIST":float64,"TCP_countries":float64,"TCP_countries_DIST":float64,"DNS_countries":float64,"DNS_countries_DIST":float64,"SSDEEP":int,"IMPHASH":int,"APT":int,"NUM_APT":int})


#print list(df)
#exit()

#Quitamos el SSDEEP pq dice que tiene campos nulos
#df = pd.read_csv(input_file, header = 0, sep=';', dtype={"MD5":object,"FIRST_SEEN":object,"SIZE":int,"NUM_PACKERS":float64,"PACKERS_BIN":float64,"PACKERS_BIN_DIST":float64,"MALWARE_TYPE":object,"NUM_IMPORTS":float64,"IMPORTS_BIN":float64,"IMPORTS_BINARY_DIST":float64,"HAS_OVERLAYS":int,"SUSPICIOUS_DLLS":float64,"SUSPICIOUS_DLLS_DIST":float64,"ANTIDEBUG_BINARY":float64,"ANTIDEBUG_BINARY_DIST":float64,"NUM_LANG":float64,"LANG_BINARY":float64,"LANG_BINARY_DIST":float64,"API_BINARY":float64,"API_BINARY_DIST":float64,"RESOURCE_NUM":int,"SERVICES_BINARY":float64,"SERVICES_BINARY_DIST":float64,"all_files_binary":float64,"all_files_binary_DIST":float64,"all_opened_files_binary":float64,"all_opened_files_binary_DIST":float64,"all_written_files_binary":float64,"all_written_files_binary_DIST":float64,"all_deleted_files_binary":float64,"all_deleted_files_binary_DIST":float64,"all_read_files_binary":float64,"all_read_files_binary_DIST":float64,"UDP_Countries":float64,"UDP_Countries_DIST":float64,"TCP_countries":float64,"TCP_countries_DIST":float64,"DNS_countries":float64,"DNS_countries_DIST":float64})


###Separamos los campos que no nos interesan y campos objetivo
malwaretypes=df.MALWARE_TYPE
isapt=df.APT
print isapt.sum()
numapt=df.NUM_APT

###Quitamos las cabeceras de los campos objetivo
values=malwaretypes.values.astype(np.int64)
values2=isapt.values.astype(np.int64)
values3=numapt.values.astype(np.int64)

print df.shape

import datetime
def convert_to_year(date_in_some_format):
	datetime_object = datetime.datetime.strptime(date_in_some_format, '%d/%m/%Y %H:%M')
	totalmins=(datetime_object-datetime.datetime(1970,1,1)).total_seconds()/60
	return int(totalmins)


def convert_malware_type_to_new_fields(row,valor):
   if int(row['MALWARE_TYPE']) == int(valor) :
      return 1;
   else:
      return 0;

print df.shape

#EXPLOTAMOS MALWARE_TYPE EN TIPOS:
df['isTypeUnknown'] = df.apply (lambda row: convert_malware_type_to_new_fields(row,-1), axis=1)
df['isOtherType'] = df.apply (lambda row: convert_malware_type_to_new_fields(row,0), axis=1)
df['isTrojan'] = df.apply (lambda row: convert_malware_type_to_new_fields(row,1), axis=1)
df['isWorm'] = df.apply (lambda row: convert_malware_type_to_new_fields(row,2), axis=1)
df['isBackdoor'] = df.apply (lambda row: convert_malware_type_to_new_fields(row,3), axis=1)
df['isRootkit'] = df.apply (lambda row: convert_malware_type_to_new_fields(row,4), axis=1)
df['isSpyware'] = df.apply (lambda row: convert_malware_type_to_new_fields(row,5), axis=1)

del df['MALWARE_TYPE']

df['DATE'] = df['FIRST_SEEN'].apply(convert_to_year)
del df['FIRST_SEEN']

###Quitamos los campos que no nos interesan
#del df['APT']
del df['MD5']
del df['NUM_APT']
del df['DEVEL_GROUP']
del df['DEVEL_COUNTRY']
del df['IMPHASH']
del df['SSDEEP']
del df['DATE']

####Quitamos los campos DIST
del df['PACKERS_BIN_DIST']
del df['IMPORTS_BINARY_DIST']
del df['ANTIDEBUG_BINARY_DIST']
del df['LANG_BINARY_DIST']
del df['API_BINARY_DIST']
del df['SERVICES_BINARY_DIST']
del df['all_files_binary_DIST']
del df['all_opened_files_binary_DIST']
del df['all_written_files_binary_DIST']
del df['all_deleted_files_binary_DIST']
del df['all_read_files_binary_DIST']
del df['UDP_Countries_DIST']
del df['TCP_countries_DIST']
del df['DNS_countries_DIST']
del df['SUSPICIOUS_DLLS_DIST']

del df['PACKERS_BIN']
del df['IMPORTS_BIN']
del df['ANTIDEBUG_BINARY']
del df['LANG_BINARY']
del df['API_BINARY']
del df['SERVICES_BINARY']
del df['all_files_binary']
del df['all_opened_files_binary']
del df['all_written_files_binary']
del df['all_deleted_files_binary']
del df['all_read_files_binary']
del df['UDP_Countries']
del df['TCP_countries']
del df['DNS_countries']
del df['SUSPICIOUS_DLLS']

del df['UNKNOWN1']
del df['UNKNOWN2']
del df['UNKNOWN3']
del df['UNKNOWN4']
del df['UNKNOWN5']
del df['UNKNOWN6']
del df['UNKNOWN7']
del df['UNKNOWN8']
del df['UNKNOWN9']
del df['UNKNOWN10']
del df['UNKNOWN11']
del df['UNKNOWN12']
del df['UNKNOWN13']
del df['UNKNOWN15']
del df['UNKNOWN16']

###Quitamos los features duplicados de imports y SUSP_APIs
del df ['IMPORTS_BIN_CheckRemoteDebuggerPresent']
del df ['IMPORTS_BIN_Connect']
del df ['SUSP_API_CreateDirectoryA']
del df ['IMPORTS_BIN_CreateFileA']
del df ['SUSP_API_CreateProcessA']
del df ['SUSP_API_CreateRemoteThread']
del df ['IMPORTS_BIN_CreateThread']
del df ['IMPORTS_BIN_DeleteFileA']
del df ['IMPORTS_BIN_FindFirstFileA']
del df ['IMPORTS_BIN_GetCommandLineA']
del df ['SUSP_API_GetCurrentProcess']
del df ['SUSP_API_GetCurrentProcessId']
del df ['SUSP_API_GetFileAttributesA']
del df ['IMPORTS_BIN_GetFileSize']
del df ['IMPORTS_BIN_GetFileSizeEx']
del df ['IMPORTS_BIN_GetModuleFileNameA']
del df ['IMPORTS_BIN_GetModuleHandleA']
del df ['IMPORTS_BIN_GetProcAddress']
del df ['IMPORTS_BIN_GetStartupInfoA']
del df ['SUSP_API_GetTempPathA']
del df ['IMPORTS_BIN_GetThreadContext']
del df ['IMPORTS_BIN_GetTickCount']
del df ['IMPORTS_BIN_GetVersionExA']
del df ['SUSP_API_IsDebuggerPresent']
del df ['IMPORTS_BIN_LdrLoadDll']
del df ['IMPORTS_BIN_LoadLibraryA']
del df ['SUSP_API_MapViewOfFile']
del df ['SUSP_API_RegCloseKey']
del df ['IMPORTS_BIN_RegCreateKeyExA']
del df ['IMPORTS_BIN_RegOpenKeyExA']
del df ['IMPORTS_BIN_ShellExecuteA']
del df ['IMPORTS_BIN_Sleep']
del df ['IMPORTS_BIN_TerminateProcess']
del df ['IMPORTS_BIN_UnhandledExceptionFilter']
del df ['IMPORTS_BIN_VirtualAlloc']
del df ['IMPORTS_BIN_VirtualFree']
del df ['IMPORTS_BIN_VirtualProtect']
del df ['IMPORTS_BIN_WriteFile']
del df ['IMPORTS_BIN_WriteProcessMemory']

#TAMBIEN SE QUITA SIZE POR PETICION DE ADOLFO
del df['SIZE']

packer_columns = [col for col in df if col.startswith('PACKER_')]
imports_columns = [col for col in df if col.startswith('IMPORTS_BIN_')]
suspapi_columns = [col for col in df if col.startswith('SUSP_API_')]
UDP_columns = [col for col in df if col.startswith('UDP_')]
DNS_columns = [col for col in df if col.startswith('DNS_')]
TCP_columns = [col for col in df if col.startswith('TCP_')]
READ_columns = [col for col in df if col.startswith('READ_')]
OPENED_columns = [col for col in df if col.startswith('OPENED_')]
DELETED_columns = [col for col in df if col.startswith('DELETED_')]
ALLFILES_columns = [col for col in df if col.startswith('ALL_FILES_')]
SERVICES_columns = [col for col in df if col.startswith('SERVICES_')]
ANTIDEBUG_columns = [col for col in df if col.startswith('ANTIDEBUG_')]
SUSPDLLS_columns = [col for col in df if col.startswith('SUSP_DLLS_')]

#df=df[:40]
#df=df.iloc[:,1215:1225]

#Quitamos los NaN en los packers
for col in packer_columns:
    df[col] = df[col].mask(df[col].isnull(), 0)

#Quitamos los NaN en los IMPORTS
for col in imports_columns:
    df[col] = df[col].mask(df[col].isnull(), 0)

#Quitamos los NaN en los SUSP_API
for col in suspapi_columns:
    df[col] = df[col].mask(df[col].isnull(), 0)

#Quitamos los NaN en los UDP_ 
for col in UDP_columns:
    df[col] = df[col].mask(df[col].isnull(), 0)

#Quitamos los NaN en los DNS
for col in DNS_columns:
    df[col] = df[col].mask(df[col].isnull(), 0)

#Quitamos los NaN en los TCP
for col in TCP_columns:
    df[col] = df[col].mask(df[col].isnull(), 0)

#Quitamos los NaN en los READ_FILES
for col in READ_columns:
    df[col] = df[col].mask(df[col].isnull(), 0)

#Quitamos los NaN en los OPENED_FILES
for col in OPENED_columns:
    df[col] = df[col].mask(df[col].isnull(), 0)

#Quitamos los NaN en los ALL_FILES
for col in ALLFILES_columns:
    df[col] = df[col].mask(df[col].isnull(), 0)

#Quitamos los NaN en los DELETED_FILES
for col in DELETED_columns:
    df[col] = df[col].mask(df[col].isnull(), 0)

#Quitamos los NaN en los SERVICES
for col in SERVICES_columns:
    df[col] = df[col].mask(df[col].isnull(), 0)

#Quitamos los NaN en los ANTIDEBUG
for col in ANTIDEBUG_columns:
    df[col] = df[col].mask(df[col].isnull(), 0)

#Quitamos los NaN en los SUSP_DLLS
for col in SUSPDLLS_columns:
    df[col] = df[col].mask(df[col].isnull(), 0)
###1962 FEATURES
df.to_csv(r'../datasets/reducido_a_1962_features_without_NaNs.csv', sep=';')

#Pero tengo los campos ya reducidos en otro fichero:
reduced_input_file="../datasets/reducido_a_1962_features_without_NaNs.csv"

reduced_df = pd.read_csv(reduced_input_file, header = 0, sep=';')

print "El df reducido tiene:"
print reduced_df.shape
#df = pd.read_csv(input_file, header = 0, sep=';', dtype={"MD5":object,"FIRST_SEEN":object,"SIZE":int,"NUM_PACKERS":float64,"PACKERS_BIN":float64,"PACKERS_BIN_DIST":float64,"MALWARE_TYPE":object,"NUM_IMPORTS":float64,"IMPORTS_BIN":float64,"IMPORTS_BINARY_DIST":float64,"HAS_OVERLAYS":int,"SUSPICIOUS_DLLS":float64,"SUSPICIOUS_DLLS_DIST":float64,"ANTIDEBUG_BINARY":float64,"ANTIDEBUG_BINARY_DIST":float64,"NUM_LANG":float64,"LANG_BINARY":float64,"LANG_BINARY_DIST":float64,"API_BINARY":float64,"API_BINARY_DIST":float64,"RESOURCE_NUM":int,"SERVICES_BINARY":float64,"SERVICES_BINARY_DIST":float64,"all_files_binary":float64,"all_files_binary_DIST":float64,"all_opened_files_binary":float64,"all_opened_files_binary_DIST":float64,"all_written_files_binary":float64,"all_written_files_binary_DIST":float64,"all_deleted_files_binary":float64,"all_deleted_files_binary_DIST":float64,"all_read_files_binary":float64,"all_read_files_binary_DIST":float64,"UDP_Countries":float64,"UDP_Countries_DIST":float64,"TCP_countries":float64,"TCP_countries_DIST":float64,"DNS_countries":float64,"DNS_countries_DIST":float64,"SSDEEP":int,"IMPHASH":int,"APT":int,"NUM_APT":int})

print df.shape
#print "Numero de Nans:"
#print df.isna().sum()

print "imports-packers-lang-resource_num"
with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also
    print(df['NUM_LANG'])
print df[df.NUM_PACKERS==0.636].shape[0]

df_lang=df.loc[(df["NUM_LANG"]>1) & (df["NUM_LANG"]<2)]
print df_lang.shape[0]
print df[df.NUM_IMPORTS==0.0].shape[0]
print df[df.RESOURCE_NUM==0].shape[0]

df_apt=df.loc[(df["APT"]==1)]
print df_apt.shape

print df_apt[df_apt.NUM_PACKERS==0.636].shape[0]

df_lang2=df_apt.loc[(df_apt["NUM_LANG"]>1) & (df_apt["NUM_LANG"]<2)]
print df_lang2.shape[0]
print df_apt[df_apt.NUM_IMPORTS==0.0].shape[0]
print df_apt[df_apt.RESOURCE_NUM==0].shape[0]


exit()
numpy.set_printoptions(threshold=numpy.nan)
##################################
# IMPUTAMOS VALORES VACIOS
##################################
#X=joblib.load("../tmp/dataset_limpiado.joblib.pkl",  mmap_mode='r')
#X=joblib.load("../tmp/dataset_mas_completo_sin_imphash2.joblib.pkl",  mmap_mode='r')

from sklearn.impute import SimpleImputer
#imp = SimpleImputer(strategy="most_frequent")
imp = SimpleImputer(strategy="mean")
print df.shape
X=imp.fit_transform(df)
print X.shape
imputado=X
imputado2=X[:150,:150]

joblib.dump(X, "../tmp/ultimo_dataset_imputado.joblib.pkl")
#exit()

##################################
# EXTRA TREE IMPORTANCES
##################################
print("=================================")
print("====Extra Tree importances=======")
print("=================================")
print("Usando el clasificador Extra Tree")

headers=list(df)
headersArray=array(array(headers))

#model=tree.ExtraTreesClassifier()
model=tree.ExtraTreeClassifier()

#values2 es una columna de 0s y 1s sobre esa muestra es de un APT o no.
model.fit(X,values2)

scores=np.column_stack((headersArray,model.feature_importances_))
mat_sort = scores[scores[:,1].astype(np.float).argsort()]
joblib.dump(mat_sort, "../tmp/scores_extratree_sindate.pkl", compress=9)
print(mat_sort)



#importances = model.feature_importances_
#std = np.std([tree.feature_importances_ for tree in model.estimators_], axis=0)
#indices = np.argsort(importances)[::-1]

# Print the feature ranking
#print("Feature ranking:")

#for f in range(X.shape[1]):
    #print("%d. feature %d (%f)" % (f + 1, indices[f], importances[indices[f]]))

# Plot the feature importances of the forest
#plt.figure()
#plt.title("Feature importances")
#plt.bar(range(X.shape[1]), importances[indices],
#       color="r", yerr=std[indices], align="center")
#plt.xticks(range(X.shape[1]), indices)
#plt.xlim([-1, X.shape[1]])
#plt.show()


# PRINT DECISSION TREE
#dot_data = tree.export_graphviz(model, out_file=None)
#dot_data = tree.export_graphviz(model, out_file=None, max_depth=4, feature_names=headers, class_names=['Malware','APT'], filled=True, rounded=True, special_characters=True)
#graph = graphviz.Source(dot_data)
#graph.render("mi_dataset")

#export_graphviz(model, out_file=dot_data, max_depth=4, feature_names=headers, class_names=['Malware','APT'], filled=True, rounded=True, special_characters=True)
#graph = graphviz.Source(dot_data)
#graph.render("arbol_extraTree")

exit()

##################################
# FEATURE SELECTION
##################################
print("===================================================")
print("====Remove features with low variance (>0,9)=======")
print("===================================================")
headers=list(df)
headersArray=array(array(headers))

#for thres in [ 0.05, 0.1, 0.15, 0.2, 0.3, 0.4, 0.5 ]:	
	#print ("PARA THRES=", thres)
	#Sel=VarianceThreshold(threshold=thres)
	#fitt=Sel.fit(X)
	#scores=np.column_stack((headersArray,fitt.get_support()))
	#support=fitt.get_support()
	#New_X=Sel.fit_transform(X)
	#mat_sort = scores[scores[:,1].argsort()]
	#print (np.sum(support))
	#print (np.count_nonzero(support))

Sel=VarianceThreshold(threshold=.1)
fitt=Sel.fit(X)
New_X=Sel.fit_transform(X)
print df.shape
print X.shape


#print headersArray.shape
#print("Son estos:")

scores=np.column_stack((headersArray,fitt.get_support(), fitt.variances_))

#print(scores.argsort())

##MAT_SORT CONTIENE UNA LISTA DE 1956 CAMPOS EN LOS QUE UNOS 135 TIENEN EL VALOR A TRUE Y SON LOS QUE SE DEBEN MANTENER (Con threshold > 0.9)
mat_sort = scores[scores[:,1].argsort()]
print(mat_sort)
print(mat_sort.shape)
joblib.dump(mat_sort, "../tmp/scores_varianza_sindate.pkl", compress=9)

exit()

##################################
# CHI2
##################################
print(" ")
print("====================================================")
print("====Univariate Feature Selection (Chi2, k=10)=======")
print("====================================================")

from sklearn.feature_selection import SelectPercentile
#chi_results=SelectKBest(chi2,'all').fit(imputado,values2)
chi_results=SelectKBest(chi2,100).fit(imputado,values2)
#chi_results=SelectPercentile(chi2,percentile=10).fit(imputado,values2)
#New_X=SelectKBest(chi2,2).fit_transform(imputado,values2)

headers=list(df)
headersArray=array(array(headers))
print("==Los scores de cada campo son==")
print(chi_results.scores_)
#print(New_X.scores_)
scores=np.column_stack((headersArray,chi_results.scores_))
mat_sort = scores[scores[:,1].astype(np.float).argsort()]
print(mat_sort)
print(" ")
exit()

##################################
# DECISION TREE IMPORTANCES
##################################
print("=================================")
print("====Decission Tree classifier====")
print("=================================")
print("Usando el clasificador Decission Tree")
from sklearn import tree

modelDT=tree.DecisionTreeClassifier()
#model.fit(df,values2)
modelDT.fit(X,values2)
scores=np.column_stack((headersArray,modelDT.feature_importances_))

mat_sort = scores[scores[:,1].astype(np.float).argsort()]
joblib.dump(mat_sort, "../tmp/scores_decission_sindate.pkl", compress=9)
print(mat_sort)

####################################
# FINAL ACCOUNTING
####################################

all_scores=np.column_stack((headersArray,chi_results.scores_, fitt.get_support(),  model.feature_importances_, modelDT.feature_importances_))

print (all_scores)
variances=fitt.get_support()

for index in range(0,chi_results.scores_.shape[1]):
	n=0
	if chi_results.scores_[index]!=nan:
		n=n+1;

	if variances[index]>0.0:
		n=n+1;

	if model.feature_importances_ > 0.0:
		n=n+1;

	if modelDT.feature_importances_ > 0.0:
		n=n+1;

	final_values[index]=n;

all_scores_final=np.column_stack((all_scores, final_values))

print all_scores_final
