##################################
# IMPORTS
##################################
import numpy as np
from numpy import *
import pandas as pd
from time import time
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from matplotlib.ticker import NullFormatter
from sklearn import manifold
from sklearn.manifold import TSNE
from sklearn.feature_selection import VarianceThreshold
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
from sklearn import datasets
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.externals import joblib 
from sklearn import tree
import sys
import numpy
from sklearn.externals.six import StringIO
import pydot
import graphviz
##################################
# PREPARATIVOS
##################################

###Leemos el dataset
#input_file="../datasets/csv_a_empezar_desde_cero_v2_sin_cols21y37_con52_y_49_46_y43_40_34_31_28_24_17_14_10_final_con_cabecera_quitamos_5_muestras.csv"
input_file="../datasets/csv_a_empezar_desde_cero_v2_sin_cols21y37_con52_y_49_46_y43_40_34_31_28_24_17_14_10_final_con_cabecera_quitamos_5_muestras_develgroup_country_new_headers.csv"

df = pd.read_csv(input_file, header = 0, sep=';', dtype={"MD5":object,"FIRST_SEEN":object,"SIZE":int,"NUM_PACKERS":float64,"PACKERS_BIN":float64,"PACKERS_BIN_DIST":float64,"MALWARE_TYPE":object,"NUM_IMPORTS":float64,"IMPORTS_BIN":float64,"IMPORTS_BINARY_DIST":float64,"HAS_OVERLAYS":int,"SUSPICIOUS_DLLS":float64,"SUSPICIOUS_DLLS_DIST":float64,"ANTIDEBUG_BINARY":float64,"ANTIDEBUG_BINARY_DIST":float64,"NUM_LANG":float64,"LANG_BINARY":float64,"LANG_BINARY_DIST":float64,"API_BINARY":float64,"API_BINARY_DIST":float64,"RESOURCE_NUM":int,"SERVICES_BINARY":float64,"SERVICES_BINARY_DIST":float64,"all_files_binary":float64,"all_files_binary_DIST":float64,"all_opened_files_binary":float64,"all_opened_files_binary_DIST":float64,"all_written_files_binary":float64,"all_written_files_binary_DIST":float64,"all_deleted_files_binary":float64,"all_deleted_files_binary_DIST":float64,"all_read_files_binary":float64,"all_read_files_binary_DIST":float64,"UDP_Countries":float64,"UDP_Countries_DIST":float64,"TCP_countries":float64,"TCP_countries_DIST":float64,"DNS_countries":float64,"DNS_countries_DIST":float64,"SSDEEP":int,"IMPHASH":int,"APT":int,"NUM_APT":int})

#print list(df)
#exit()

#Quitamos el SSDEEP pq dice que tiene campos nulos
#df = pd.read_csv(input_file, header = 0, sep=';', dtype={"MD5":object,"FIRST_SEEN":object,"SIZE":int,"NUM_PACKERS":float64,"PACKERS_BIN":float64,"PACKERS_BIN_DIST":float64,"MALWARE_TYPE":object,"NUM_IMPORTS":float64,"IMPORTS_BIN":float64,"IMPORTS_BINARY_DIST":float64,"HAS_OVERLAYS":int,"SUSPICIOUS_DLLS":float64,"SUSPICIOUS_DLLS_DIST":float64,"ANTIDEBUG_BINARY":float64,"ANTIDEBUG_BINARY_DIST":float64,"NUM_LANG":float64,"LANG_BINARY":float64,"LANG_BINARY_DIST":float64,"API_BINARY":float64,"API_BINARY_DIST":float64,"RESOURCE_NUM":int,"SERVICES_BINARY":float64,"SERVICES_BINARY_DIST":float64,"all_files_binary":float64,"all_files_binary_DIST":float64,"all_opened_files_binary":float64,"all_opened_files_binary_DIST":float64,"all_written_files_binary":float64,"all_written_files_binary_DIST":float64,"all_deleted_files_binary":float64,"all_deleted_files_binary_DIST":float64,"all_read_files_binary":float64,"all_read_files_binary_DIST":float64,"UDP_Countries":float64,"UDP_Countries_DIST":float64,"TCP_countries":float64,"TCP_countries_DIST":float64,"DNS_countries":float64,"DNS_countries_DIST":float64})


###Separamos los campos que no nos interesan y campos objetivo
malwaretypes=df.MALWARE_TYPE
isapt=df.APT
numapt=df.NUM_APT

###Quitamos las cabeceras de los campos objetivo
values=malwaretypes.values.astype(np.int64)
values2=isapt.values.astype(np.int64)
values3=numapt.values.astype(np.int64)

print values2.shape
print df.shape

import datetime
def convert_to_year(date_in_some_format):
	datetime_object = datetime.datetime.strptime(date_in_some_format, '%d/%m/%Y %H:%M')
	totalmins=(datetime_object-datetime.datetime(1970,1,1)).total_seconds()/60
	return int(totalmins)


def convert_malware_type_to_new_fields(row,valor):
   if int(row['MALWARE_TYPE']) == int(valor) :
      return 1;
   else:
      return 0;

print df.shape

#EXPLOTAMOS MALWARE_TYPE EN TIPOS:
df['isTypeUnknown'] = df.apply (lambda row: convert_malware_type_to_new_fields(row,-1), axis=1)
df['isOtherType'] = df.apply (lambda row: convert_malware_type_to_new_fields(row,0), axis=1)
df['isTrojan'] = df.apply (lambda row: convert_malware_type_to_new_fields(row,1), axis=1)
df['isWorm'] = df.apply (lambda row: convert_malware_type_to_new_fields(row,2), axis=1)
df['isBackdoor'] = df.apply (lambda row: convert_malware_type_to_new_fields(row,3), axis=1)
df['isRootkit'] = df.apply (lambda row: convert_malware_type_to_new_fields(row,4), axis=1)
df['isSpyware'] = df.apply (lambda row: convert_malware_type_to_new_fields(row,5), axis=1)

del df['MALWARE_TYPE']


df['DATE'] = df['FIRST_SEEN'].apply(convert_to_year)
#print (df['Date'])

del df['FIRST_SEEN']

###Quitamos los campos que no nos interesan
#del df['MALWARE_TYPE']
del df['APT']
del df['MD5']
del df['NUM_APT']
del df['DEVEL_GROUP']
del df['DEVEL_COUNTRY']

del df['IMPHASH']
del df['SSDEEP']
del df['DATE']

####Quitamos los campos DIST
del df['PACKERS_BIN_DIST']
del df['IMPORTS_BINARY_DIST']
del df['ANTIDEBUG_BINARY_DIST']
del df['LANG_BINARY_DIST']
del df['API_BINARY_DIST']
del df['SERVICES_BINARY_DIST']
del df['all_files_binary_DIST']
del df['all_opened_files_binary_DIST']
del df['all_written_files_binary_DIST']
del df['all_deleted_files_binary_DIST']
del df['all_read_files_binary_DIST']
del df['UDP_Countries_DIST']
del df['TCP_countries_DIST']
del df['DNS_countries_DIST']
del df['SUSPICIOUS_DLLS_DIST']

del df['PACKERS_BIN']
del df['IMPORTS_BIN']
del df['ANTIDEBUG_BINARY']
del df['LANG_BINARY']
del df['API_BINARY']
del df['SERVICES_BINARY']
del df['all_files_binary']
del df['all_opened_files_binary']
del df['all_written_files_binary']
del df['all_deleted_files_binary']
del df['all_read_files_binary']
del df['UDP_Countries']
del df['TCP_countries']
del df['DNS_countries']
del df['SUSPICIOUS_DLLS']

del df['UNKNOWN1']
del df['UNKNOWN2']
del df['UNKNOWN3']
del df['UNKNOWN4']
del df['UNKNOWN5']
del df['UNKNOWN6']
del df['UNKNOWN7']
del df['UNKNOWN8']
del df['UNKNOWN9']
del df['UNKNOWN10']
del df['UNKNOWN11']
del df['UNKNOWN12']
del df['UNKNOWN13']
del df['UNKNOWN15']
del df['UNKNOWN16']

###Quitamos los features duplicados de imports y SUSP_APIs
del df ['IMPORTS_BIN_CheckRemoteDebuggerPresent']
del df ['IMPORTS_BIN_Connect']
del df ['SUSP_API_CreateDirectoryA']
del df ['IMPORTS_BIN_CreateFileA']
del df ['SUSP_API_CreateProcessA']
del df ['SUSP_API_CreateRemoteThread']
del df ['IMPORTS_BIN_CreateThread']
del df ['IMPORTS_BIN_DeleteFileA']
del df ['IMPORTS_BIN_FindFirstFileA']
del df ['IMPORTS_BIN_GetCommandLineA']
del df ['SUSP_API_GetCurrentProcess']
del df ['SUSP_API_GetCurrentProcessId']
del df ['SUSP_API_GetFileAttributesA']
del df ['IMPORTS_BIN_GetFileSize']
del df ['IMPORTS_BIN_GetFileSizeEx']
del df ['IMPORTS_BIN_GetModuleFileNameA']
del df ['IMPORTS_BIN_GetModuleHandleA']
del df ['IMPORTS_BIN_GetProcAddress']
del df ['IMPORTS_BIN_GetStartupInfoA']
del df ['SUSP_API_GetTempPathA']
del df ['IMPORTS_BIN_GetThreadContext']
del df ['IMPORTS_BIN_GetTickCount']
del df ['IMPORTS_BIN_GetVersionExA']
del df ['SUSP_API_IsDebuggerPresent']
del df ['IMPORTS_BIN_LdrLoadDll']
del df ['IMPORTS_BIN_LoadLibraryA']
del df ['SUSP_API_MapViewOfFile']
del df ['SUSP_API_RegCloseKey']
del df ['IMPORTS_BIN_RegCreateKeyExA']
del df ['IMPORTS_BIN_RegOpenKeyExA']
del df ['IMPORTS_BIN_ShellExecuteA']
del df ['IMPORTS_BIN_Sleep']
del df ['IMPORTS_BIN_TerminateProcess']
del df ['IMPORTS_BIN_UnhandledExceptionFilter']
del df ['IMPORTS_BIN_VirtualAlloc']
del df ['IMPORTS_BIN_VirtualFree']
del df ['IMPORTS_BIN_VirtualProtect']
del df ['IMPORTS_BIN_WriteFile']
del df ['IMPORTS_BIN_WriteProcessMemory']

#TAMBIEN SE QUITA SIZE POR PETICION DE ADOLFO
del df['SIZE']

#df=df[:40]
#df=df.iloc[:,1215:1225]

numpy.set_printoptions(threshold=numpy.nan)
##################################
# IMPUTAMOS VALORES VACIOS
##################################
#X=joblib.load("../tmp/dataset_limpiado.joblib.pkl",  mmap_mode='r')
#X=joblib.load("../tmp/dataset_mas_completo_sin_imphash2.joblib.pkl",  mmap_mode='r')

from sklearn.impute import SimpleImputer
#imp = SimpleImputer(strategy="most_frequent")
imp = SimpleImputer(strategy="mean")
print df.shape
X=imp.fit_transform(df)
print X.shape
imputado=X
imputado2=X[:150,:150]

joblib.dump(X, "../tmp/ultimo_dataset_imputado.joblib.pkl")
#exit()

##################################
# CHI2
##################################
print(" ")
print("====================================================")
print("====Univariate Feature Selection (Chi2, k=10)=======")
print("====================================================")

from sklearn.feature_selection import SelectPercentile
#chi_results=SelectKBest(chi2,'all').fit(imputado,values2)
chi_results=SelectKBest(chi2,400).fit(imputado,values2)
#chi_results=SelectPercentile(chi2,percentile=10).fit(imputado,values2)
#New_X=SelectKBest(chi2,2).fit_transform(imputado,values2)

headers=list(df)
headersArray=array(array(headers))
print("==Los scores de cada campo son==")
#print(chi_results.scores_)
#print(New_X.scores_)
scores=np.column_stack((headersArray,chi_results.scores_))
mat_sort = scores[scores[:,1].astype(np.float).argsort()]
print (mat_sort)
lista = pd.DataFrame({'header': headersArray, 'chi': chi_results.scores_})
lista.sort_values(by=['chi'], inplace=True, ascending=False)

sorted_chis=lista['chi']
sorted_headers=lista['header']

short_sorted_chis=sorted_chis[0:200]
short_sorted_headers=sorted_headers[0:200]

plt.locator_params(axis='y', nbins=16)

plt.figure()

plt.title("Chi2 Feature scores")
###plt.bar(range(X.shape[1]), importances[indices], color="r", yerr=std[indices], align="center")
plt.bar(range(len(short_sorted_chis)), short_sorted_chis, color="b", align="center")
###plt.xticks(range(X.shape[1]), indices)
plt.xticks(range(len(short_sorted_chis)), short_sorted_headers, rotation=80)

ax = plt.gca()
ax.tick_params(axis = 'x', which = 'major', labelsize = 6, width=1)
ax.tick_params(axis = 'x', which = 'minor', labelsize = 6, width=1)

iii=0
for tick in ax.xaxis.get_major_ticks():
    if iii%2==0:
        tick.label.set_color('red')
    if iii%2==1:
        tick.label.set_color('black')
    iii+=1

plt.xlim([0, len(short_sorted_chis)])
plt.show()




###plt.xticks(range(X.shape[1]), indices)
#plt.xticks(range(len(short_sorted_importances)), short_sorted_headers, rotation=80)

#ax = plt.gca()
#ax.tick_params(axis = 'x', which = 'major', labelsize = 5, width=2)
#ax.tick_params(axis = 'x', which = 'minor', labelsize = 5, width=2)

#iii=0
#for tick in ax.xaxis.get_major_ticks(): 
    #if iii%2==0:
        #tick.label.set_color('red')
    #if iii%2==1:
        #tick.label.set_color('black')
    #iii+=1

#plt.xlim([0, len(short_sorted_importances)])

#joblib.dump(mat_sort, "../tmp/scores_extratree_sindate.pkl", compress=9)
#print(mat_sort)



#importances = model.feature_importances_
#std = np.std([tree.feature_importances_ for tree in model.estimators_], axis=0)
#indices = np.argsort(importances)[::-1]

# Print the feature ranking
#print("Feature ranking:")

#for f in range(X.shape[1]):
    #print("%d. feature %d (%f)" % (f + 1, indices[f], importances[indices[f]]))

# Plot the feature importances of the forest
#plt.figure()
#plt.title("Feature importances")
#plt.bar(range(X.shape[1]), importances[indices],
#       color="r", yerr=std[indices], align="center")
#plt.xticks(range(X.shape[1]), indices)
#plt.xlim([-1, X.shape[1]])
#plt.show()


# PRINT DECISSION TREE
#dot_data = tree.export_graphviz(model, out_file=None)
#dot_data = tree.export_graphviz(model, out_file=None, max_depth=4, feature_names=headers, class_names=['Malware','APT'], filled=True, rounded=True, special_characters=True)
#graph = graphviz.Source(dot_data)
#graph.render("mi_dataset")

#export_graphviz(model, out_file=dot_data, max_depth=4, feature_names=headers, class_names=['Malware','APT'], filled=True, rounded=True, special_characters=True)
#graph = graphviz.Source(dot_data)
#graph.render("arbol_extraTree")

exit()


##################################
# DECISION TREE IMPORTANCES
##################################
print("=================================")
print("====Decission Tree classifier====")
print("=================================")
print("Usando el clasificador Decission Tree")
from sklearn import tree

modelDT=tree.DecisionTreeClassifier()
#model.fit(df,values2)
modelDT.fit(X,values2)
scores=np.column_stack((headersArray,modelDT.feature_importances_))

mat_sort = scores[scores[:,1].astype(np.float).argsort()]
joblib.dump(mat_sort, "../tmp/scores_decission_sindate.pkl", compress=9)
print(mat_sort)

####################################
# FINAL ACCOUNTING
####################################

all_scores=np.column_stack((headersArray,chi_results.scores_, fitt.get_support(),  model.feature_importances_, modelDT.feature_importances_))

print (all_scores)
variances=fitt.get_support()

for index in range(0,chi_results.scores_.shape[1]):
	n=0
	if chi_results.scores_[index]!=nan:
		n=n+1;

	if variances[index]>0.0:
		n=n+1;

	if model.feature_importances_ > 0.0:
		n=n+1;

	if modelDT.feature_importances_ > 0.0:
		n=n+1;

	final_values[index]=n;

all_scores_final=np.column_stack((all_scores, final_values))

print all_scores_final
