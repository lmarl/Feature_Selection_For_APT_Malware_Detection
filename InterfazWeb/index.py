
####################################
#    IMPORTS
####################################
import numpy as np
from numpy import *
#pandas
import pandas as pd
from pandas.plotting import scatter_matrix
#matplotlib
from mpl_toolkits.mplot3d import Axes3D
from matplotlib.ticker import NullFormatter
import matplotlib.image as mpimg
import matplotlib.pyplot as plt
import matplotlib.cm as cm
#streamlit
import streamlit as st
#sklearn
from sklearn.metrics import silhouette_samples, silhouette_score
from sklearn.feature_selection import VarianceThreshold
from sklearn.feature_selection import SelectPercentile
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
from sklearn.exceptions import DataConversionWarning
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.impute import SimpleImputer
from sklearn.cluster import KMeans
from sklearn import decomposition
from sklearn import preprocessing
from sklearn import manifold
from sklearn import datasets
from sklearn import tree
import sklearn.metrics as mt
import os.path
from os import path
###RANDOM FOREST###########
from sklearn.ensemble import RandomForestClassifier

###LR###########
from sklearn.linear_model import LogisticRegression

###KNN###########
from sklearn import neighbors, datasets
from sklearn.model_selection import cross_val_predict
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import cross_val_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import precision_score, recall_score

###SVM##########
from sklearn import svm
from sklearn.model_selection import cross_val_predict
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import cross_val_score

###AUC###########
from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve, auc
from sklearn.metrics import classification_report

#other
from time import time
import warnings
import datetime
import joblib
import sys
import os

currentdir = os.path.dirname(os.path.realpath(__file__))
parentdir = os.path.dirname(currentdir)
sys.path.append(parentdir)

from aptta import machine_learning as mc
from aptta import web

###############################################################
# CONFIGURATION
###############
input_file= "../datasets/dataset.csv"
second_input_file= "../datasets/validation_dataset.csv"
data_dir="../tmp"

#####################
# PRELIMINAR TASKS
#####################
#os.system("clear")

print ("Reading CSVs...")
#First dataset
df = pd.read_csv(input_file, header = 0, sep=';') 
apttypes=df.APT
isapt=apttypes
numapt=df.NUM_APT
values2=apttypes.values.astype(np.int64)
values3=numapt.values.astype(np.int64)
del df['APT']
del df['NUM_APT']
num=1944
primer_length=19457
df=df.iloc[0:primer_length,0:num]
atributos=list(df)
atributos2=atributos[0:num]
atributos2=atributos

#Validation dataset
segundo_df = pd.read_csv(second_input_file, header = 0) 
#segundo_apttypes=segundo_df.APT
segundo_isapt=segundo_df.isAPT
#segundo_numapt=segundo_df.NUM_APT
#segundo_values2=segundo_apttypes.values.astype(np.int64)
#segundo_values3=segundo_numapt.values.astype(np.int64)
segundo_length=4875
segundo_values2=segundo_isapt.values.astype(np.int64)
del segundo_df['isAPT']
del segundo_df['MD5']



df1=df[['ANTIDEBUG_CheckRemoteDebuggerPresent','ANTIDEBUG_FindWindowExA','ANTIDEBUG_FindWindowExW','ANTIDEBUG_GetWindowThreadProcessId','ANTIDEBUG_%IsDebuggerPresent','ANTIDEBUG_OutputDebugStringA','ANTIDEBUG_Process32NextW','DELETED_FILES_C:','DELETED_FILES_C:/DOCUME~1/<USER>~1/LOCALS~1/Temp','DELETED_FILES_C:/Documents_and_Settings/All_Users/Application_Data','DELETED_FILES_C:/Documents_and_Settings/All_Users/Start_Menu/Programs','DELETED_FILES_C:/Documents_and_Settings/<USER>/Desktop','DELETED_FILES_c:/windows/temp','DNS_CH','DNS_CZ','DNS_IL','DNS_NL','DNS_SY','IMPORTS_BIN_bind','IMPORTS_BIN_CallNextHookEx','IMPORTS_BIN_CharNextA','IMPORTS_BIN_CloseHandle','IMPORTS_BIN_CopyFile','IMPORTS_BIN_CreateDirectoryA','IMPORTS_BIN_CreateMutex','IMPORTS_BIN_CreateSymbolicLink','IMPORTS_BIN_DeleteCriticalSection','IMPORTS_BIN_DispatchMessageA','IMPORTS_BIN_EnterCriticalSection','IMPORTS_BIN_FindNextFile','IMPORTS_BIN_FindWindow','IMPORTS_BIN_FreeLibrary','IMPORTS_BIN_GetClientRect','IMPORTS_BIN_GetCurrentThreadId','IMPORTS_BIN_GetDC','IMPORTS_BIN_GetDeviceCaps','IMPORTS_BIN_GetDlgItem','IMPORTS_BIN_GetFileInformation','IMPORTS_BIN_GetFileTime','IMPORTS_BIN_GetFileType','IMPORTS_BIN_GetLastError','IMPORTS_BIN_GetLongPathName','IMPORTS_BIN_GetSystemDirectoryA','IMPORTS_BIN_GetTempPathA','IMPORTS_BIN_GetVersion','IMPORTS_BIN_GetWindowLongA','IMPORTS_BIN_GetWindowsDirectory','IMPORTS_BIN_GlobalAlloc','IMPORTS_BIN_GlobalFree','IMPORTS_BIN_GlobalUnlock','IMPORTS_BIN_HeapFree','IMPORTS_BIN_inet_addr','IMPORTS_BIN_InitializeCriticalSection','IMPORTS_BIN_IsDebuggerPresent','IMPORTS_BIN_LeaveCriticalSection','IMPORTS_BIN_LoadLibrary','IMPORTS_BIN_lstrcpynA','IMPORTS_BIN_MapViewOfFile','IMPORTS_BIN_OpenFile','IMPORTS_BIN_RegOpenKey','IMPORTS_BIN_RegQueryValueExA','IMPORTS_BIN_RegSetValueExA','IMPORTS_BIN_SelectObject','IMPORTS_BIN_SetFilePointer','IMPORTS_BIN_SetWindowLongA','IMPORTS_BIN_ShowWindow','IMPORTS_BIN_TlsGetValue','IMPORTS_BIN_wsprintfA','OPENED_FILES_C:','OPENED_FILES_C:/Documents_and_Settings/<USER>','OPENED_FILES_C:/Documents_and_Settings/<USER>/Start_Menu/Programs/Startup','OPENED_FILES_C:/WINDOWS/Registration','OPENED_FILES_C:/WINDOWS/system','OPENED_FILES_C:/WINDOWS/system32','OPENED_FILES_c:/windows/temp','OPENED_FILES_//./Global','PACKER_Armadillo','PACKER_NullSoft','PACKER_UPX','READ_FILES_C:','READ_FILES_C:/Documents_and_Settings/<USER>/Application_Data/Microsoft/Internet_Explorer/Quick_Launch','SERVICES_AudioSrv','SERVICES_Ias','SERVICES_NetDDEdsdm','SERVICES_Ntmssvc','SERVICES_RASMAN','SERVICES_Win32','SERVICES_WmiApSrv','SUSP_API_CopyFileA','SUSP_API_CopyFileW','SUSP_API_CreateFileA','SUSP_API_CreateProcessAsUserA','SUSP_API_CreateProcessW','SUSP_API_CreateProcessWithLogonW','SUSP_API_CreateServiceW','SUSP_API_CreateThread','SUSP_API_CryptEncrypt','SUSP_API_DeleteFileA','SUSP_API_DisconnectNamedPipe','SUSP_API_FindFirstFileA','SUSP_API_FindFirstFileExW','SUSP_API_GetCommandLineW','SUSP_API_GetModuleFileNameA','SUSP_API_GetProcAddress','SUSP_API_GetStartupInfoA','SUSP_API_GetStartupInfoW','SUSP_API_GetTempPathW','SUSP_API_GetVersionExA','SUSP_API_HttpSendRequestExA','SUSP_API_HttpSendRequestExW','SUSP_API_InternetConnectA','SUSP_API_InternetOpenA','SUSP_API_InternetOpenUrlA','SUSP_API_InternetQueryOptionA','SUSP_API_LoadLibraryA','SUSP_API_LoadLibraryExA','SUSP_API_LoadLibraryW','SUSP_API_LockResource','SUSP_API_OpenFileMappingW','SUSP_API_OpenProcess','SUSP_API_RegCreateKeyExA','SUSP_API_RegCreateKeyW','SUSP_API_RegDeleteValueA','SUSP_API_RegEnumKeyA','SUSP_API_RegEnumKeyExA','SUSP_API_RegOpenKeyExA','SUSP_API_ShellExecuteExA','SUSP_API_SleepEx','SUSP_API_StartServiceA','SUSP_API_StartServiceCtrlDispatcherW','SUSP_API_StartServiceW','SUSP_API_VirtualFree','SUSP_API_WriteFile','SUSP_API_WriteProcessMemory','SUSP_DLLS_ADVAPI32.dll','SUSP_DLLS_comdlg32.dll','SUSP_DLLS_crypt32.dll','SUSP_DLLS_GDI32.DLL','SUSP_DLLS_HAL.dll','SUSP_DLLS_KERNEL32.dll','SUSP_DLLS_MFC42.DLL','SUSP_DLLS_MSVFW32.dll','SUSP_DLLS_ntoskrnl.exe','SUSP_DLLS_OLE32.DLL','SUSP_DLLS_OPENGL32.DLL','SUSP_DLLS_SECUR32.dll','SUSP_DLLS_wininet.dll','SUSP_DLLS_WSOCK32.DLL','TCP_AE','TCP_CH','TCP_CN','TCP_DE','TCP_KR','TCP_MD','TCP_NL','TCP_US','UDP_CL','UDP_GB','UDP_MD','UDP_MT','UDP_US','ANTIDEBUG_TerminateProcess','ANTIDEBUG_UnhandledExceptionFilter','DNS_US','HAS_OVERLAYS','IMPORTS_BIN_CreateFile','IMPORTS_BIN_CreateProcessA','IMPORTS_BIN_DeleteFile','IMPORTS_BIN_DeleteObject','IMPORTS_BIN_ExitProcess','IMPORTS_BIN_FileSize','IMPORTS_BIN_FindFirstFile','IMPORTS_BIN_GetACP','IMPORTS_BIN_GetCPInfo','IMPORTS_BIN_GetCurrentProcess','IMPORTS_BIN_GetFileAttributes','IMPORTS_BIN_GetFileAttributesA','IMPORTS_BIN_GetFullPathName','IMPORTS_BIN_GetShortPathName','IMPORTS_BIN_GetStdHandle','IMPORTS_BIN_GetTempFileName','IMPORTS_BIN_GlobalLock','IMPORTS_BIN_HeapAlloc','IMPORTS_BIN_lstrlenA','IMPORTS_BIN_MoveFile','IMPORTS_BIN_MultiByteToWideChar','IMPORTS_BIN_ReadFile','IMPORTS_BIN_RegCloseKey','IMPORTS_BIN_RtlUnwind','IMPORTS_BIN_SetErrorMode','IMPORTS_BIN_SetFileAttributes','IMPORTS_BIN_SetFileTime','IMPORTS_BIN_WaitForSingleObject','IMPORTS_BIN_WideCharToMultiByte','NUM_IMPORTS','NUM_PACKERS','OPENED_FILES_//.','OPENED_FILES_C:/DOCUME~1/<USER>~1/LOCALS~1/Temp','OPENED_FILES_C:/WINDOWS','OPENED_FILES_//./PIPE','READ_FILES_C:/WINDOWS/Registration','READ_FILES_C:/WINDOWS/system32','RESOURCE_NUM','SUSP_API_CreateFileW','SUSP_API_FindNextFileA','SUSP_API_FindResourceA','SUSP_API_GetCommandLineA','SUSP_API_GetFileSize','SUSP_API_GetModuleHandleA','SUSP_API_GetTempFileNameA','SUSP_API_GetTickCount','SUSP_API_RegDeleteKeyA','SUSP_API_ShellExecuteA','SUSP_API_Sleep','SUSP_API_VirtualAlloc','SUSP_API_VirtualProtect','SUSP_DLLS_COMCTL32.DLL','SUSP_DLLS_Msvcrt.dll','SUSP_DLLS_OLEAUT32.DLL','SUSP_DLLS_Shell32.dll','SUSP_DLLS_SHLWAPI.dll','SUSP_DLLS_USER32.dll','SUSP_DLLS_version.dll','isTypeUnknown','isOtherType','isTrojan','isBackdoor','isRootkit','isSpyware']]
df=df1

segundo_df1=segundo_df[['ANTIDEBUG_CheckRemoteDebuggerPresent','ANTIDEBUG_FindWindowExA','ANTIDEBUG_FindWindowExW','ANTIDEBUG_GetWindowThreadProcessId','ANTIDEBUG_%IsDebuggerPresent','ANTIDEBUG_OutputDebugStringA','ANTIDEBUG_Process32NextW','DELETED_FILES_C:','DELETED_FILES_C:/DOCUME~1/<USER>~1/LOCALS~1/Temp','DELETED_FILES_C:/Documents_and_Settings/All_Users/Application_Data','DELETED_FILES_C:/Documents_and_Settings/All_Users/Start_Menu/Programs','DELETED_FILES_C:/Documents_and_Settings/<USER>/Desktop','DELETED_FILES_c:/windows/temp','DNS_CH','DNS_CZ','DNS_IL','DNS_NL','DNS_SY','IMPORTS_BIN_bind','IMPORTS_BIN_CallNextHookEx','IMPORTS_BIN_CharNextA','IMPORTS_BIN_CloseHandle','IMPORTS_BIN_CopyFile','IMPORTS_BIN_CreateDirectoryA','IMPORTS_BIN_CreateMutex','IMPORTS_BIN_CreateSymbolicLink','IMPORTS_BIN_DeleteCriticalSection','IMPORTS_BIN_DispatchMessageA','IMPORTS_BIN_EnterCriticalSection','IMPORTS_BIN_FindNextFile','IMPORTS_BIN_FindWindow','IMPORTS_BIN_FreeLibrary','IMPORTS_BIN_GetClientRect','IMPORTS_BIN_GetCurrentThreadId','IMPORTS_BIN_GetDC','IMPORTS_BIN_GetDeviceCaps','IMPORTS_BIN_GetDlgItem','IMPORTS_BIN_GetFileInformation','IMPORTS_BIN_GetFileTime','IMPORTS_BIN_GetFileType','IMPORTS_BIN_GetLastError','IMPORTS_BIN_GetLongPathName','IMPORTS_BIN_GetSystemDirectoryA','IMPORTS_BIN_GetTempPathA','IMPORTS_BIN_GetVersion','IMPORTS_BIN_GetWindowLongA','IMPORTS_BIN_GetWindowsDirectory','IMPORTS_BIN_GlobalAlloc','IMPORTS_BIN_GlobalFree','IMPORTS_BIN_GlobalUnlock','IMPORTS_BIN_HeapFree','IMPORTS_BIN_inet_addr','IMPORTS_BIN_InitializeCriticalSection','IMPORTS_BIN_IsDebuggerPresent','IMPORTS_BIN_LeaveCriticalSection','IMPORTS_BIN_LoadLibrary','IMPORTS_BIN_lstrcpynA','IMPORTS_BIN_MapViewOfFile','IMPORTS_BIN_OpenFile','IMPORTS_BIN_RegOpenKey','IMPORTS_BIN_RegQueryValueExA','IMPORTS_BIN_RegSetValueExA','IMPORTS_BIN_SelectObject','IMPORTS_BIN_SetFilePointer','IMPORTS_BIN_SetWindowLongA','IMPORTS_BIN_ShowWindow','IMPORTS_BIN_TlsGetValue','IMPORTS_BIN_wsprintfA','OPENED_FILES_C:','OPENED_FILES_C:/Documents_and_Settings/<USER>','OPENED_FILES_C:/Documents_and_Settings/<USER>/Start_Menu/Programs/Startup','OPENED_FILES_C:/WINDOWS/Registration','OPENED_FILES_C:/WINDOWS/system','OPENED_FILES_C:/WINDOWS/system32','OPENED_FILES_c:/windows/temp','OPENED_FILES_//./Global','PACKER_Armadillo','PACKER_NullSoft','PACKER_UPX','READ_FILES_C:','READ_FILES_C:/Documents_and_Settings/<USER>/Application_Data/Microsoft/Internet_Explorer/Quick_Launch','SERVICES_AudioSrv','SERVICES_Ias','SERVICES_NetDDEdsdm','SERVICES_Ntmssvc','SERVICES_RASMAN','SERVICES_Win32','SERVICES_WmiApSrv','SUSP_API_CopyFileA','SUSP_API_CopyFileW','SUSP_API_CreateFileA','SUSP_API_CreateProcessAsUserA','SUSP_API_CreateProcessW','SUSP_API_CreateProcessWithLogonW','SUSP_API_CreateServiceW','SUSP_API_CreateThread','SUSP_API_CryptEncrypt','SUSP_API_DeleteFileA','SUSP_API_DisconnectNamedPipe','SUSP_API_FindFirstFileA','SUSP_API_FindFirstFileExW','SUSP_API_GetCommandLineW','SUSP_API_GetModuleFileNameA','SUSP_API_GetProcAddress','SUSP_API_GetStartupInfoA','SUSP_API_GetStartupInfoW','SUSP_API_GetTempPathW','SUSP_API_GetVersionExA','SUSP_API_HttpSendRequestExA','SUSP_API_HttpSendRequestExW','SUSP_API_InternetConnectA','SUSP_API_InternetOpenA','SUSP_API_InternetOpenUrlA','SUSP_API_InternetQueryOptionA','SUSP_API_LoadLibraryA','SUSP_API_LoadLibraryExA','SUSP_API_LoadLibraryW','SUSP_API_LockResource','SUSP_API_OpenFileMappingW','SUSP_API_OpenProcess','SUSP_API_RegCreateKeyExA','SUSP_API_RegCreateKeyW','SUSP_API_RegDeleteValueA','SUSP_API_RegEnumKeyA','SUSP_API_RegEnumKeyExA','SUSP_API_RegOpenKeyExA','SUSP_API_ShellExecuteExA','SUSP_API_SleepEx','SUSP_API_StartServiceA','SUSP_API_StartServiceCtrlDispatcherW','SUSP_API_StartServiceW','SUSP_API_VirtualFree','SUSP_API_WriteFile','SUSP_API_WriteProcessMemory','SUSP_DLLS_ADVAPI32.dll','SUSP_DLLS_comdlg32.dll','SUSP_DLLS_crypt32.dll','SUSP_DLLS_GDI32.DLL','SUSP_DLLS_HAL.dll','SUSP_DLLS_KERNEL32.dll','SUSP_DLLS_MFC42.DLL','SUSP_DLLS_MSVFW32.dll','SUSP_DLLS_ntoskrnl.exe','SUSP_DLLS_OLE32.DLL','SUSP_DLLS_OPENGL32.DLL','SUSP_DLLS_SECUR32.dll','SUSP_DLLS_wininet.dll','SUSP_DLLS_WSOCK32.DLL','TCP_AE','TCP_CH','TCP_CN','TCP_DE','TCP_KR','TCP_MD','TCP_NL','TCP_US','UDP_CL','UDP_GB','UDP_MD','UDP_MT','UDP_US','ANTIDEBUG_TerminateProcess','ANTIDEBUG_UnhandledExceptionFilter','DNS_US','HAS_OVERLAYS','IMPORTS_BIN_CreateFile','IMPORTS_BIN_CreateProcessA','IMPORTS_BIN_DeleteFile','IMPORTS_BIN_DeleteObject','IMPORTS_BIN_ExitProcess','IMPORTS_BIN_FileSize','IMPORTS_BIN_FindFirstFile','IMPORTS_BIN_GetACP','IMPORTS_BIN_GetCPInfo','IMPORTS_BIN_GetCurrentProcess','IMPORTS_BIN_GetFileAttributes','IMPORTS_BIN_GetFileAttributesA','IMPORTS_BIN_GetFullPathName','IMPORTS_BIN_GetShortPathName','IMPORTS_BIN_GetStdHandle','IMPORTS_BIN_GetTempFileName','IMPORTS_BIN_GlobalLock','IMPORTS_BIN_HeapAlloc','IMPORTS_BIN_lstrlenA','IMPORTS_BIN_MoveFile','IMPORTS_BIN_MultiByteToWideChar','IMPORTS_BIN_ReadFile','IMPORTS_BIN_RegCloseKey','IMPORTS_BIN_RtlUnwind','IMPORTS_BIN_SetErrorMode','IMPORTS_BIN_SetFileAttributes','IMPORTS_BIN_SetFileTime','IMPORTS_BIN_WaitForSingleObject','IMPORTS_BIN_WideCharToMultiByte','NUM_IMPORTS','NUM_PACKERS','OPENED_FILES_//.','OPENED_FILES_C:/DOCUME~1/<USER>~1/LOCALS~1/Temp','OPENED_FILES_C:/WINDOWS','OPENED_FILES_//./PIPE','READ_FILES_C:/WINDOWS/Registration','READ_FILES_C:/WINDOWS/system32','RESOURCE_NUM','SUSP_API_CreateFileW','SUSP_API_FindNextFileA','SUSP_API_FindResourceA','SUSP_API_GetCommandLineA','SUSP_API_GetFileSize','SUSP_API_GetModuleHandleA','SUSP_API_GetTempFileNameA','SUSP_API_GetTickCount','SUSP_API_RegDeleteKeyA','SUSP_API_ShellExecuteA','SUSP_API_Sleep','SUSP_API_VirtualAlloc','SUSP_API_VirtualProtect','SUSP_DLLS_COMCTL32.DLL','SUSP_DLLS_Msvcrt.dll','SUSP_DLLS_OLEAUT32.DLL','SUSP_DLLS_Shell32.dll','SUSP_DLLS_SHLWAPI.dll','SUSP_DLLS_USER32.dll','SUSP_DLLS_version.dll','isTypeUnknown','isOtherType','isTrojan','isBackdoor','isRootkit','isSpyware']]
segundo_df=segundo_df1


##################################
# IMPUTE EMPTY VALUES
##################################
def impute_empty_values(df):
    print ("Imputting...")
    imp = SimpleImputer(strategy="most_frequent")
    X=imp.fit_transform(df)
    return X

##################################
# Standarization
##################################
def standardize(df):
    print ("Standardizing...")
    X = preprocessing.scale(df)
    return X

##################################
# Normalization (0 to 1)
##################################
def normalize(df):
    headers=list(df)
    x = df.values #returns a numpy array
    min_max_scaler = preprocessing.MinMaxScaler()
    x_scaled = min_max_scaler.fit_transform(x)
    X = pd.DataFrame(x_scaled)
    return X

####################################
#   INTERNAL METHODS
####################################
def get_redundant_pairs(df):
    '''Get diagonal and lower triangular pairs of correlation matrix'''
    pairs_to_drop = set()
    cols = df.columns
    for i in range(0, df.shape[1]):
        for j in range(0, i+1):
            pairs_to_drop.add((cols[i], cols[j]))
    return pairs_to_drop

def get_top_abs_correlations(df, n=5):
    au_corr = df.corr().abs().unstack()
    labels_to_drop = get_redundant_pairs(df)
    au_corr = au_corr.drop(labels=labels_to_drop, errors="ignore").sort_values(ascending=False)
    return au_corr[0:n]


##################################
# PCA
##################################
def generate_pca(components, barplot_title, scatterplot_title):
    #Configuracion
    data_filename = data_dir + '/pca_3D_238.npy'
    output_file='../results/DoubleFigPCAByAPTandMalware_238.png'
    tmp_file="../tmp/dataset_238.joblib.pkl"

    if (data=='Primer dataset'):
        X=impute_empty_values(df)
        length=primer_length
    elif (data=='Segundo dataset'):
        X=impute_empty_values(segundo_df)
        length=segundo_length

    J=standardize(X)
    X=J

    # PCA
    print ("Calculating PCA...")
    pca = decomposition.PCA(n_components=components)
    Y=pca.fit_transform(X)
    print ("Done!")

    # PLOTTING
    fig = plt.figure(figsize=(15,15))
    if components==3:
       ax = fig.add_subplot(projection='3d')
    else:
       ax = fig.add_subplot()

    colors=['red', 'green']
    target_names=['Malware','APT']
    target=[0, 1]
    lista=[0,1]

    # Gráfico de barras (Finalmente eliminado)
    barfig, barax = plt.subplots(figsize = [6, 4.34])
#    barax.title.set_text(barplot_title)
#    barax.bar(range(1, components + 1), pca.explained_variance_ratio_)

    # Gráfico de dispersion 
    if components==3:
        for color, i, target_name, pos in zip(colors, target, target_names, lista):
            indices=[]
            paraimprimir=[]

            for index in range(0,length):
                if values2[index]==i:
                    indices=np.append(indices,index)

            indices2=indices.astype(np.int64)
            paraimprimir=Y[indices2]
            if pos!=1:
                ax.scatter(paraimprimir[:, 0], paraimprimir[:, 1], paraimprimir[:, 2], cmap=plt.cm.Spectral, s=1, c=color, label=target_names[pos])

        for color, i, target_name, pos in zip(colors, target, target_names, lista):
            indices=[]
            paraimprimir=[]

            for index in range(0,length):
                if values2[index]==i:
                   indices=np.append(indices,index)

            indices2=indices.astype(np.int64)
            paraimprimir=Y[indices2]
            if pos==1:
                    ax.scatter(paraimprimir[:, 0], paraimprimir[:, 1], paraimprimir[:, 2], cmap=plt.cm.Spectral, s=9, c=color, label=target_names[pos])

    elif components==2:
        #LO MOSTRAMOS POR MALWARE/APT
        for color, i, target_name, pos in zip(colors, target, target_names, lista):
            indices=[]
            paraimprimir=[]

            print (Y.size)
            for ndx in range(0,length):
                if values2[ndx]==i:
                    indices=np.append(indices,ndx)

            indices2=indices.astype(np.int64)
            paraimprimir=Y[indices2]
            ax.scatter(paraimprimir[:, 0], paraimprimir[:, 1], cmap=plt.cm.Spectral, s=1, c=color, label=target_names[pos])

    text= "Lost variance is:"
    res = " ".join([str(i) for i in pca.explained_variance_])
    text=text+res
    st.text(text)

    text2="Variance ratio is:"
    res2= " ".join([str(i) for i in pca.explained_variance_ratio_])
    text2=text2+res2
    st.text(text2)

    np.save(data_filename, Y)
    return barfig, fig

# función para representar un gráfico de dispersión en 2D
def scatter_2d(self, scatterplot_title, x):
        scatterfig, scatterax = plt.subplots()
        scatterax = plt.axes(title = scatterplot_title)
        common = scatterax.scatter(x[self.common, 0], x[self.common, 1], c = 'green', marker = '.')
        apt = scatterax.scatter(x[self.malicious, 0], x[self.malicious, 1], c = 'red', marker = '.')
        scatterax.set_xlabel('x')
        scatterax.set_ylabel('y')
        scatterax.legend([common, apt], ['Habitual', 'Malicioso'])

        return scatterfig

# Función para representar un gráfico de dispersión en 3D
def scatter_3d(self, scatterplot_title, x):
        scatterfig, scatterax = plt.subplots(figsize = [7, 5])
        scatterax = plt.axes(projection='3d', title = scatterplot_title)
        common = scatterax.scatter3D(x[self.common, 0], x[self.common, 1], x[self.common, 2], c = 'green', marker = '.')
        apt = scatterax.scatter3D(x[self.malicious, 0], x[self.malicious, 1], x[self.malicious, 2], c = 'red', marker = '.')
        scatterax.set_xlabel('x')
        scatterax.set_ylabel('y')
        scatterax.set_zlabel('z')
        scatterax.legend([common, apt], ['Habitual', 'Malicioso'], loc = 'upper right', bbox_to_anchor=(0.25,0.35))


##################################
# TSNE
##################################
def generate_tsne(components, scatterplot_title, perplexity, learning_rate, n_iter):
    
        #configuration
        tmp_file="../tmp/dataset_standardized.joblib.pkl"
        output_file='../results/DoubleFigTSNEByAPTandMalware_1941_verdes_bigdots.png'
        colors=['red', 'green']
        target_names=['Malware','APT']
        target=[0, 1]
        lista=[0,1]

        if (data=='Primer dataset'):
            data_filename = data_dir + '/tsne_1941_3D_e.npy'
            X=impute_empty_values(df)
            length=primer_length
        elif (data=='Segundo dataset'):
            data_filename = data_dir + '/tsne_1941_3D_f.npy'
            X=impute_empty_values(segundo_df)
            length=segundo_length

        J=standardize(X)
        X=J

        # Aplicación de T-SNE
        tsne = manifold.TSNE(n_components = components, perplexity = perplexity, learning_rate = learning_rate, n_iter = n_iter)
        Y = tsne.fit_transform(X)
        print ("Finished...")
        np.save(data_filename, Y)

        # Creación del gráfico de dispersión
        fig = plt.figure(figsize=(15,15))
        if components == 2:
            for color, i, target_name, pos in zip(colors, target, target_names, lista):
                indices=[]
                paraimprimir=[]

                for index in range(0,length):
                        if values2[index]==i:
                                indices=np.append(indices,index)

                indices2=indices.astype(np.int64)
                paraimprimir=Y[indices2]
                plt.scatter(paraimprimir[:, 0], paraimprimir[:, 1], cmap=plt.cm.Spectral, s=1, c=color, label=target_names[pos])

        elif components == 3:
             ax = fig.add_subplot(projection='3d')
             for color, i, target_name, pos in zip(colors, target, target_names, lista):
                indices=[]
                paraimprimir=[]

                for index in range(0,length):
                        if values2[index]==i:
                                indices=np.append(indices,index)

                indices2=indices.astype(np.int64)
                paraimprimir=Y[indices2]
                if (color=='green'):
                        ax.scatter(paraimprimir[:, 0], paraimprimir[:, 1], paraimprimir[:, 2], cmap=plt.cm.Spectral, s=4, c=color, label=target_names[pos])
                else:
                        ax.scatter(paraimprimir[:, 0], paraimprimir[:, 1], paraimprimir[:, 2], cmap=plt.cm.Spectral, s=1, c=color, label=target_names[pos])

        plt.title("t-SNE (perplexity: %.2g)" % (perplexity))
        return fig


def load_tsne(components):
        if (data=='Primer dataset'):
            data_filename = data_dir + '/tsne_1941_3D_e.npy'
            length=primer_length
        elif (data=='Segundo dataset'):
            data_filename = data_dir + '/tsne_1941_3D_f.npy'
            length=segundo_length

        Y = np.load(data_filename)
        colors=['red', 'green']
        target_names=['Malware','APT']
        target=[0, 1]
        lista=[0,1]
        # Creación del gráfico de dispersión
        fig = plt.figure(figsize=(15,15))
        if components == 2:
            for color, i, target_name, pos in zip(colors, target, target_names, lista):
                indices=[]
                paraimprimir=[]

                for index in range(0,length):
                        if values2[index]==i:
                                indices=np.append(indices,index)

                indices2=indices.astype(np.int64)
                paraimprimir=Y[indices2]
                plt.scatter(paraimprimir[:, 0], paraimprimir[:, 1], cmap=plt.cm.Spectral, s=1, c=color, label=target_names[pos])

        elif components == 3:
             ax = fig.add_subplot(projection='3d')
             for color, i, target_name, pos in zip(colors, target, target_names, lista):
                indices=[]
                paraimprimir=[]

                for index in range(0,length):
                        if values2[index]==i:
                                indices=np.append(indices,index)

                indices2=indices.astype(np.int64)
                paraimprimir=Y[indices2]
                if (color=='green'):
                        ax.scatter(paraimprimir[:, 0], paraimprimir[:, 1], paraimprimir[:, 2], cmap=plt.cm.Spectral, s=4, c=color, label=target_names[pos])
                else:
                        ax.scatter(paraimprimir[:, 0], paraimprimir[:, 1], paraimprimir[:, 2], cmap=plt.cm.Spectral, s=1, c=color, label=target_names[pos])

        plt.title("t-SNE (perplexity: %.2g)" % (perplexity))
        return fig

##################################
# Heatmap
##################################
def calculate_heatmap():

    if (data=='Primer dataset'):
            output_file="../results/correlation_heatmap_1941.png"
            correlations=df.corr()
    elif (data=='Segundo dataset'):
            output_file="../results/correlation_heatmap_validacion.png"
            correlations=segundo_df.corr()

    fig = plt.figure(figsize=(36,36)) #Image Size
    #fig = plt.figure() #Image Size
    ax = fig.add_subplot()

    cax = ax.matshow(correlations, vmin=-1, vmax=1)
    fig.colorbar(cax)

    font = {'family': 'serif',
        'color':  'black',
        'weight': 'normal',
        'size': 4,
        }

    ticks = np.arange(0,num-2,1)
    ax.set_xticks(ticks)
    ax.set_yticks(ticks)
    ax.set_xticklabels(atributos2, fontdict=font)
    ax.set_yticklabels(atributos2, fontdict=font)

    plt.xticks(rotation=80)

    fig.savefig(output_file)

    return fig, correlations

##################################
# CHI2
##################################
def calculate_chi2():
    #configuration
    num_bars=200

    print("Calculating CHI2...")
    if (data=='Primer dataset'):
            output_file="../results/codo_chi2_line_1941.png"
            chi_results=SelectKBest(chi2,'all').fit(df,values2)
            headers=list(df)
    elif (data=='Segundo dataset'):
            output_file="../results/codo_chi2_line_validation.png"
            chi_results=SelectKBest(chi2,'all').fit(segundo_df,values2)
            headers=list(segundo_df)

    #chi_results=SelectKBest(chi2,'all').fit(imputado,values2)
    #chi_results=SelectPercentile(chi2,percentile=10).fit(imputado,values2)
    print("Done!!!")

    headersArray=array(array(headers))

    scores=np.column_stack((headersArray,chi_results.scores_))
    sorted_scores = scores[(-scores[:,1].astype(np.float)).argsort()]

    lista = pd.DataFrame({'header': headersArray, 'chi': chi_results.scores_})
    lista.sort_values(by=['chi'], inplace=True, ascending=False)

    sorted_chis=lista['chi']
    sorted_headers=lista['header']

    short_sorted_chis=sorted_chis[0:num_bars]
    short_sorted_headers=sorted_headers[0:num_bars]
    shorted_scores=sorted_scores[:20]

    plt.locator_params(axis='y', nbins=16)

    fig=plt.figure()
    plt.title("Chi2 Feature scores")
    #ax = fig.add_subplot()
    #ax2 = fig.add_subplot()

    #BAR Plot
    ###plt.bar(range(X.shape[1]), importances[indices], color="r", yerr=std[indices], align="center")
    plt.bar(range(len(short_sorted_chis)), short_sorted_chis, color="b", align="center")
    plt.xticks(range(len(short_sorted_chis)), short_sorted_headers, rotation=80)

    ax = plt.gca()
    ax.tick_params(axis = 'x', which = 'major', labelsize = 6, width=1)
    ax.tick_params(axis = 'x', which = 'minor', labelsize = 6, width=1)

    iii=0
    for tick in ax.xaxis.get_major_ticks():
        if iii%2==0:
            tick.label.set_color('red')
        if iii%2==1:
            tick.label.set_color('black')
        iii+=1

    plt.xlim([0, len(short_sorted_chis)])

    #LINE PLot
    plt.title("Chi2 Feature importances")
    plt.plot(range(len(short_sorted_chis)), short_sorted_chis, color="b")

    plt.savefig(output_file)

    return fig,shorted_scores 

##################################
# EXTRA TREE IMPORTANCES
##################################
def calculate_extraTree():

    #configuration
    output_file="../results/elbow_extra_tree_1941_fields_and_56_bars.png"
    num_bars=56

    print ("Calculating extra tree importances...")
    headers=list(df)
    headersArray=array(array(headers))

    #model=tree.ExtraTreeClassifier()
    model=ExtraTreesClassifier(n_estimators=250)

    model.fit(df,values2)
    importances=model.feature_importances_
    indices = np.argsort(importances)[::-1]

    lista = pd.DataFrame({'header': headersArray, 'importance': importances})
    lista.sort_values(by=['importance'], inplace=True, ascending=False)

    sorted_importances=lista['importance']
    sorted_headers=lista['header']

    short_sorted_importances=sorted_importances[0:num_bars]
    short_sorted_headers=sorted_headers[0:num_bars]

    scores=np.column_stack((headersArray,model.feature_importances_))
    sorted_scores = scores[(-scores[:,1].astype(np.float)).argsort()]
    sorted_scores2 = sorted_scores[:20]

    # PLOTTING
    fig=plt.figure()
    plt.title("ExtraTree Feature importances")
    plt.bar(range(len(short_sorted_importances)), short_sorted_importances, color="r", align="center")
    ###plt.xticks(range(X.shape[1]), indices)
    plt.xticks(range(len(short_sorted_importances)), short_sorted_headers, rotation=80)

    ax = plt.gca()
    ax.tick_params(axis = 'x', which = 'major', labelsize = 5, width=2)
    ax.tick_params(axis = 'x', which = 'minor', labelsize = 5, width=2)

    iii=0
    for tick in ax.xaxis.get_major_ticks():
        if iii%2==0:
            tick.label.set_color('red')
        if iii%2==1:
            tick.label.set_color('black')
        iii+=1

    plt.xlim([0, len(short_sorted_importances)])

    #Asked for a LINE plot
    plt.plot(range(len(short_sorted_importances)), short_sorted_importances, color="r")

    plt.savefig(output_file, bbox_inches='tight')

    return fig, sorted_scores2


##################################
# CLUSTERING
##################################

def apply_kmeans(clusters, x, predict):
        kmeans = KMeans(n_clusters = clusters, random_state = self.random_state)
        kmeans.fit(x)

        return kmeans.predict(predict), kmeans.cluster_centers_

def evaluate_clustering2(plot_title, n_clusters_):

    data_dir="../tmp"
    if (data=='Primer dataset'):
            data_filename = data_dir + '/tsne_1941_3D_e.npy'
            length=primer_length
            reduced_data=standardize(df)
    elif (data=='Segundo dataset'):
            data_filename = data_dir + '/tsne_1941_3D_f.npy'
            length=segundo_length
            reduced_data=standardize(segundo_df)

    ###Calculamos el KMeans:
    kmeans = KMeans(n_clusters=n_clusters_, random_state=0).fit(reduced_data)
    df3=pd.DataFrame(data=reduced_data)
    predict=kmeans.predict(reduced_data)
    centroides=kmeans.cluster_centers_

    df3['estimated_cluster']=pd.Series(predict, index=df3.index)
    values4=pd.Series(predict, index=df3.index)

    Y = np.load(data_filename)

    fig = plt.figure(figsize=(15, 15))

    ax = fig.add_subplot(projection='3d')

#ax.scatter(reduced_data[:, 0], reduced_data[:, 1], reduced_data[:,2], cmap=plt.cm.Spectral, s=2, c=kmeans.labels_.astype(float))
    ax.scatter(Y[:, 0], Y[:, 1], Y[:,2], cmap=plt.cm.Spectral, s=2, c=values4)

###Y pintamos los centroides
#centroids = kmeans.cluster_centers_
#ax.scatter(centroids[:, 0], centroids[:, 1], centroids[:,2],marker='x', s=169, linewidths=3, color='black', zorder=10)
#plt.title("Kmeans with %s groups" % (num+1) );
    plt.title('Estimated number of clusters: %d' % n_clusters_)

    return fig

def evaluate_clustering(plot_title, clusters):

        if (data=='Primer dataset'):
            reduced_data=standardize(df)
        elif (data=='Segundo dataset'):
            reduced_data=standardize(segundo_df)


        ###Calculamos el KNN:
        kmeans = KMeans(n_clusters=n_clusters_, random_state=0).fit(reduced_data)
        df3=pd.DataFrame(data=reduced_data)
        predict=kmeans.predict(reduced_data)
        centroides=kmeans.cluster_centers_

        #cluster_labels, _ = apply_kmeans(clusters, X, X)

        silhouette_avg, davies, calinski = self.internal_clustering_scores(self.x, cluster_labels)
        sample_silhouette_values = mt.silhouette_samples(self.x, cluster_labels)

        fig, ax = plt.subplots(figsize = [8, 6])
        ax = plt.axes(title = plot_title)
        ax.set_xlim([-0.2, 1])

        ax.set_xlabel("Coeficientes de silueta")
        ax.set_ylabel("Etiquetas de grupo")

        ax.axvline(x = silhouette_avg, color = "red", linestyle = "--")

        ax.set_yticks([])
        ax.set_xticks([-0.2, 0, 0.2, 0.4, 0.6, 0.8, 1])

        y_lower = 10
        for i in range(clusters):
            ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]

            ith_cluster_silhouette_values.sort()

            size_cluster_i = ith_cluster_silhouette_values.shape[0]
            y_upper = y_lower + size_cluster_i

            color = cm.nipy_spectral(float(i) / clusters)
            ax.fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_silhouette_values, facecolor=color, edgecolor=color, alpha=0.7)

            ax.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))

            y_lower = y_upper + 10

        return fig, silhouette_avg, davies, calinski

##################################
# SILHOUETTE ANALYSIS
##################################
def silhouette(df, n_clusters):
    tmp_file="../tmp/tsne_para_kmeans.joblib.pkl"
    if (data=='Primer dataset'):
            output_file= "../results/silhouette_1941_4clusters.png"
            X=normalize(df)
    elif (data=='Segundo dataset'):
            output_file= "../results/silhouette_validation_4clusters.png"
            X=normalize(segundo_df)

    J=standardize(X)
    X=J

    # Create a subplot with 1 row and 2 columns
    fig, (ax1, ax2) = plt.subplots(1, 2)
    fig.set_size_inches(14, 7)

    # The 1st subplot is the silhouette plot
    # The silhouette coefficient can range from -1, 1 but in this example all
    # lie within [-0.1, 1]
    ax1.set_xlim([-0.1, 1])

    # The (n_clusters+1)*10 is for inserting blank space between silhouette
    # plots of individual clusters, to demarcate them clearly.
    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])

    # Initialize the clusterer with n_clusters value and a random generator
    # seed of 10 for reproducibility.
    clusterer = KMeans(n_clusters=n_clusters, random_state=10)
    cluster_labels = clusterer.fit_predict(X)

    # The silhouette_score gives the average value for all the samples.
    # This gives a perspective into the density and separation of the formed
    # clusters
    silhouette_avg = silhouette_score(X, cluster_labels)

    print("For n_clusters =", n_clusters,
          "The average silhouette_score is :", silhouette_avg)

    # Compute the silhouette scores for each sample
    sample_silhouette_values = silhouette_samples(X, cluster_labels)

    y_lower = 10
    for i in range(n_clusters):
        # Aggregate the silhouette scores for samples belonging to
        # cluster i, and sort them
        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]

        ith_cluster_silhouette_values.sort()

        size_cluster_i = ith_cluster_silhouette_values.shape[0]
        y_upper = y_lower + size_cluster_i

        color = cm.nipy_spectral(float(i) / n_clusters)
        ax1.fill_betweenx(np.arange(y_lower, y_upper),
                          0, ith_cluster_silhouette_values,
                          facecolor=color, edgecolor=color, alpha=0.7)

        # Label the silhouette plots with their cluster numbers at the middle
        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))

        # Compute the new y_lower for next plot
        y_lower = y_upper + 10  # 10 for the 0 samples

    ax1.set_title("The silhouette plot for the various clusters.")
    ax1.set_xlabel("The silhouette coefficient values")
    ax1.set_ylabel("Cluster label")

    # The vertical line for average silhouette score of all the values
    ax1.axvline(x=silhouette_avg, color="red", linestyle="--")

    ax1.set_yticks([])  # Clear the yaxis labels / ticks
    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])

    # 2nd Plot showing the actual clusters formed
    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)
    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,
                c=colors, edgecolor='k')

    # Labeling the clusters
    centers = clusterer.cluster_centers_
    # Draw white circles at cluster centers
    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',
                c="white", alpha=1, s=200, edgecolor='k')

    for i, c in enumerate(centers):
        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,
                    s=50, edgecolor='k')

    ax2.set_title("The visualization of the clustered data.")
    ax2.set_xlabel("Feature space for the 1st feature")
    ax2.set_ylabel("Feature space for the 2nd feature")

    plt.suptitle(("Silhouette analysis for KMeans clustering on sample data "
                  "with n_clusters = %d" % n_clusters),
                 fontsize=14, fontweight='bold')

    plt.savefig(output_file)

    return fig, silhouette_avg

##################################
# CLASSIFICATION
#################N#################
def evaluate_model_instance(model_instance, use_test_folds, X_train, Y_train_APT, X_test, Y_test_APT):
        K = len(X_train)

        kappa = np.zeros(K)
        acc = np.zeros(K)
        rc = np.zeros(K)
        pr = np.zeros(K)
        f1 = np.zeros(K)

        fitted_model = model_instance.fit(X_train, Y_train_APT)  #tx, ty
        y_test_pred=cross_val_predict(fitted_model, X_test, Y_test_APT)

        # Confusion Matrix
        cm=confusion_matrix(Y_test_APT,y_test_pred)
        # Accuracy
        acc=1.0* (cm[0][0]+cm[1][1])/ num_tes
        # Prec
        pr=precision_score(Y_test_APT,y_test_pred)
        # Recall
        recall=recall_score(Y_test_APT,y_test_pred)
        # Kappa 
        kappa=mt.cohen_kappa_score(Y_test_APT,y_test_pred)
        # F1
        F1=2*pr*recall/(pr+recall)

        return kappa, acc, recall, pr, F1, cm


def classification_scores(true_y, pred_y):
        k = 100 * mt.cohen_kappa_score(true_y, pred_y)
        a = 100 * mt.accuracy_score(true_y, pred_y)
        r = 100 * mt.recall_score(true_y, pred_y)
        p = 100 * mt.precision_score(true_y, pred_y, zero_division = 0)
        f1 = 100 * mt.f1_score(true_y, pred_y)

        return k, a, r, p, f1

def tune_knn(vecinos,X_train, Y_train_APT, X_test, Y_test_APT):
        best_kappa = float('-inf')
        best_n = vecinos[0]

        for n in vecinos:
            model = neighbors.KNeighborsClassifier(n_neighbors = n)
            kappa, _, _, _, _, _ = evaluate_model_instance(model, True, X_train, Y_train_APT, X_test, Y_test_APT)

            if kappa > best_kappa:
                best_kappa = kappa
                best_n = n

        knn_file="../results/knn_optimized_parameters.bin"
        with open(knn_file, 'w') as f:
             f.write(str(best_n)+'\n')
             f.close()

        return best_n

def tune_logistic_regression(solvers, Crange, penaltyOptions, X_train, Y_train_APT, X_test, Y_test_APT):
    best_kappa = float('-inf')
    #CRange is a range between 5 and 20, but we need 0.5 to 2,0, so I'll divide it by 10
    for solver_ in solvers:
        for penalty_ in penaltyOptions[solver_]:
            for c_ in Crange:
                #st.write('Trying with solver: ' + str(solver_)+', penalty: '+str(penalty_)+' and C='+str(c_))
                model = LogisticRegression(solver=solver_, penalty=penalty_, C=c_/10)
                kappa, _, _, _, _, _ = evaluate_model_instance(model, True, X_train, Y_train_APT, X_test, Y_test_APT)

                if kappa > best_kappa:
                    best_kappa = kappa
                    best_c = c_
                    best_p = penalty_ 
                    best_s = solver_

        lr_file="../results/lr_optimized_parameters.bin"
        with open(lr_file, 'w') as f:
             f.write(str(best_c)+'\n')
             f.write(str(best_p)+'\n')
             f.write(str(best_s)+'\n')
             f.close()

        return best_c, best_p, best_s

def tune_svc(Crange, gammaRange, X_train, Y_train_APT, X_test, Y_test_APT):
    best_kappa = float('-inf')
    for gamma_ in gammaRange:
            for c_ in Crange:
                #st.write("Probando con gamma="+gamma_+" y C="+c_)
                print("Probando con gamma="+str(gamma_)+" y C="+str(c_/100))
                model=svm.SVC(kernel='linear', probability=True, C=c_/100, gamma=gamma_)
                kappa, _, _, _, _, _ = evaluate_model_instance(model, True, X_train, Y_train_APT, X_test, Y_test_APT)

                if kappa > best_kappa:
                    best_kappa = kappa
                    best_c = c_
                    best_g = gamma_ 

    svm_file="../results/svm_optimized_parameters.bin"
    with open(svm_file, 'w') as f:
             f.write(str(best_c)+'\n')
             f.write(str(best_g)+'\n')
             f.close()

    return best_c, best_g

def tune_random_forest(n_estimators, max_features, max_depth, min_samples_split, criterion, X_train, Y_train_APT, X_test, Y_test_APT):
        best_kappa = float('-inf')
        best_n = n_estimators[0]
        best_f = max_features[0]
        best_d = max_depth[0]
        best_s = min_samples_split[0]
        best_c = criterion[0]

        for n in n_estimators:
            for f in max_features:
                for d in max_depth:
                    for s in min_samples_split:
                        for c in criterion:
                            #forest_clf = RandomForestClassifier(random_state=42,n_estimators=100
                            print ("probando con n_estimators="+str(n)+", max_features="+str(f)+", max_depth="+str(d)+", min_samples_split="+str(s)+", criterion="+str(c))
                            model = RandomForestClassifier(random_state=42,n_estimators=n, max_features=f, max_depth=d, min_samples_split=s, criterion=c)
                            kappa, _, _, _, _, _ = evaluate_model_instance(model, False, X_train, Y_train_APT, X_test, Y_test_APT)

                            if kappa > best_kappa:
                                print ("===> El mejor por ahora es n_estimators="+str(n)+", max_features="+str(f)+", max_depth="+str(d)+", min_samples_split="+str(s)+", criterion="+str(c))
                                best_kappa = kappa
                                best_n = n
                                best_f = f
                                best_d = d
                                best_s = s
                                best_c = c

        #Write results in file
        rf_file="../results/rf_optimized_parameters.bin"
        with open(rf_file, 'w') as f:
             f.write(str(best_n)+'\n')
             f.write(str(best_f)+'\n')
             f.write(str(best_d)+'\n')
             f.write(str(best_s)+'\n')
             f.write(str(best_c)+'\n')
             f.close()

        return best_n, best_f, best_d, best_s, best_c

def quality_scores(fields, values):
    fig, ax = plt.subplots(figsize = (6.4, 2))
    ax.bar(fields, values, width = 0.3)

    for i in range(5):
        ax.text(i - 0.13, values[i] - 8, str(round(values[i], 2)), color='white', size = 'x-small')

    return fig


##########################################################
##########################################################

def disable(): 
    st.session_state["disabled"] = True 
    
def enable(): 
    st.session_state["disabled"] =  False
    

####################################################################################################################################################
# Carga de los datasets

# Estilo
web.define_style()
st.session_state["disabled"] = False 

# Menu de navegación
st.sidebar.markdown('<h1 class = "centered">Proyecto de Tesis</h1>', unsafe_allow_html=True)
st.sidebar.markdown('<h3 class = "justified">Análisis del malware perteneciente a ataques APT</h3>', unsafe_allow_html=True)

#page=""


    #if datasettt == 'Conjunto de datos inicial (19457 filas y 1941 características)':
    #    st.sidebar.markdown('<p class = "justified">Conjunto de datos del primer experimento con 19457 filas y 1941 características</p>', unsafe_allow_html=True)
    #    page='Primer Conjunto de datos'
    #    section = st.sidebar.selectbox(label = 'Elija la sección:', options = ('Conjunto de datos', 'Representación visual','Analisis de correlacion', 'Agrupamiento', 'Clasificación'), index = 0)
    #elif datasettt == 'Conjunto de datos de validación (Aun no funciona)':
    #    st.sidebar.markdown('<p class = "justified">Conjunto de datos de validación</p>', unsafe_allow_html=True)
    #    page='Segundo Conjunto de datos'
    #    section = st.sidebar.selectbox(label = 'Elija la sección:', options = ('Conjunto de datos', 'Representación visual','Analisis de correlacion', 'Agrupamiento', 'Clasificación'), index = 0)
#else:
# section = st.sidebar.selectbox(label = 'Elija la sección:', options = ('Conjunto de datos', 'Representación visual','Analisis de correlacion', 'Agrupamiento', 'Clasificación'), index = 0)

data = st.sidebar.radio(label = 'Elija el dataset:', options = ['Primer dataset', 'Segundo dataset'], index = 0)
if 'section' in locals() and not section == 'Conjunto de datos' and data=='Segundo dataset':
    section='Conjunto de datos'

section = st.sidebar.selectbox(label = 'Elija la sección:', options = ('Conjunto de datos', 'Representación visual','Analisis de correlacion', 'Agrupamiento', 'Clasificación'), index = 0)

if section == 'Conjunto de datos':
    st.sidebar.title(section)
    page='Conjunto de datos'
    if data=='Primer dataset':
        dataset=df
        st.sidebar.markdown('<p class = "justified">Conjunto de datos del primer experimento con 19457 filas y 1941 carcaterísticas</p>', unsafe_allow_html=True)
    if data=='Segundo dataset':
        dataset=segundo_df
        st.sidebar.markdown('<p class = "justified">Conjunto de datos de validación con 4876 filas y 1941 carcaterísticas</p>', unsafe_allow_html=True)

if section == 'Analisis de correlacion':
    st.sidebar.title(section)
    st.sidebar.markdown('<p class = "justified">Correlación entre características del conjunto de datos</p>', unsafe_allow_html=True)
    page = st.sidebar.radio(label = 'Elija la página:', options = ['Análisis de correlación', 'Análisis Chi Cuadrado', 'Analisis de importancias de Extra Tree'], index = 0)

elif section == 'Representación visual':
    st.sidebar.title(section)
    st.sidebar.markdown('<p class = "justified">Reducción de la dimensionalidad del conjunto de datos</p>', unsafe_allow_html=True)
    page = st.sidebar.radio(label = 'Elija la página:', options = ['Representación gráfica con PCA', 'Representación gráfica con T-SNE'], index = 0)
elif section == 'Agrupamiento':
    st.sidebar.title(section)
    st.sidebar.markdown('<p class = "justified">Agrupamiento usando el algoritmo K-Means con PCA</p>', unsafe_allow_html=True)
    page = st.sidebar.radio(label = 'Elija la página:', options = ['Agrupamiento K-Means', 'Análisis de Silhouette'], index = 0)
elif section == 'Clasificación':
    st.sidebar.title(section)
    st.sidebar.markdown('<p class = "justified">Evaluación del rendimiento de distintos clasificadores sobre los conjuntos de datos</p>', unsafe_allow_html=True)
    page = st.sidebar.radio(label = 'Elija la página:', options = ['Modelos de clasificación', 'Comparativa', 'AUC'], index = 0)

 # Título general
st.title(page)

########## DATASET ######################################
if page == 'Conjunto de datos':
    if data=='Primer dataset':
      with st.spinner('Loading dataset inicial...'):
        df3=dataset[:100]
        print (dataset.shape)
        st.dataframe(df3.style.format("{:.0f}"))
        b64 = web.download_dataset(dataset)
        st.markdown(f'<a class = "download" href="data:file/csv;base64,{b64}" download="dataset.csv">Descargar conjunto de datos</a>', unsafe_allow_html=True)
    if data=='Segundo dataset':
      with st.spinner('Loading dataset de validacion...'):
        df3=dataset[:100]
        print (dataset.shape)
        st.dataframe(df3.style.format("{:.0f}"))
        b64 = web.download_dataset(dataset)
        st.markdown(f'<a class = "download" href="data:file/csv;base64,{b64}" download="dataset.csv">Descargar conjunto de datos</a>', unsafe_allow_html=True)


if page == 'Segundo Conjunto de datos':
    section = st.sidebar.selectbox(label = 'Elija la sección:', options = ('Conjunto de datos', 'Representación visual','Analisis de correlacion', 'Agrupamiento', 'Clasificación'), index = 0)
    with st.spinner('Loading dataset...'):
        df3=dataset[:100]
        st.dataframe(df3.style.format("{:.0f}"))
        b64 = web.download_dataset(dataset)
        st.markdown(f'<a class = "download" href="data:file/csv;base64,{b64}" download="dataset.csv">Descargar conjunto de datos</a>', unsafe_allow_html=True)

########## PCA ######################################
elif page == 'Representación gráfica con PCA':
    dimensionality = st.selectbox(label = 'Seleccione la dimensión:', options = ('2D', '3D'), index = 0)

    if dimensionality == '2D':
        dimension = 2
    else:
        dimension = 3

    barfig, scatterfig  = generate_pca(dimension, '', '')

    st.subheader('Gráfico de dispersión')
    st.pyplot(scatterfig)

########### TSNE #####################################
elif page == 'Representación gráfica con T-SNE':
    figure2d = None
    figure3d = None

    with st.form("hyperparameters"):
        col1, col2 = st.columns(2)

        with col1:
            perplexity = st.number_input(label = 'Perplejidad (5.0 - 50.0):', min_value = 5.0, max_value = 50.0, step = 5.0, value = 30.0, disabled=st.session_state.disabled)
            iterations = st.number_input(label = 'Número de iteraciones (250 - 5000):', min_value = 250, max_value = 5000, step = 50, value = 1000, disabled=st.session_state.disabled)

        with col2:
            learning_rate = st.number_input(label = 'Coeficiente de aprendizaje (10.0 - 1000.0):', min_value = 10.0, max_value = 1000.0, step = 10.0, value = 200.0)

        submitted = st.form_submit_button("Representar")
        submittedLast = st.form_submit_button("Mostrar ultimo gráfico obtenido")

        if submitted:
            with st.spinner('Aplicando algoritmo T-SNE (Puede tardar hasta 40 minutos)...'):
                figure2d = generate_tsne(2, '', perplexity, learning_rate, iterations)
                figure3d = generate_tsne(3, '', perplexity, learning_rate, iterations)
        elif submittedLast:
                figure2d = load_tsne(2)
                figure3d = load_tsne(3)

    if figure2d != None: 
            st.subheader('Gráfico en 2D')
            st.pyplot(figure2d)

    if figure3d != None: 
            st.subheader('Gráfico en 3D')
            st.pyplot(figure3d)

##########HEATMAP######################################
elif page == 'Análisis de correlación':
    st.subheader('Análisis de correlación con 1941 características')
    with st.form("corr"):
        submitted = st.form_submit_button("Mostrar heatmap")
        submitted_Last = st.form_submit_button("Mostrar ultimo heatmap obtenido")
        if submitted:
            with st.spinner('Obteniendo el mapa de calor de correlaciones (Puede tardar 10 minutos)...'):
                corr_fig, correlations = calculate_heatmap()
            st.pyplot(corr_fig)

            with pd.option_context('display.max_rows', None, 'display.max_columns', None):
                X=correlations.abs().mean().sort_values()
                st.write ("Correlacion de cada campo ordenado: ", X)
                st.write ("Correlación media entre campos: ",X.mean())

                df_attr = pd.Series( (v for v in atributos) )
                st.write("Top 20 de correlaciones más altas: ",get_top_abs_correlations(df, 20))

        elif submitted_Last:
                output_file="../results/correlation_heatmap_1941.png"
                st.image(output_file,use_column_width=True)

##########CHI2######################################
elif page == 'Análisis Chi Cuadrado':
    st.subheader('Análisis Chi2')
    chi2_fig,sorted_scores = calculate_chi2()
    st.pyplot(chi2_fig)
    st.write("Top 20 Univariate Feature Selection (Chi2, k=10): ", sorted_scores)

########## EXTRA TREE######################################
elif page == 'Analisis de importancias de Extra Tree':
    st.subheader('Análisis Extra Tree')
    extraTree_fig, shorted_scores = calculate_extraTree()
    st.pyplot(extraTree_fig)
    st.write("Top 20 fields by Extra Tree importances:", shorted_scores)

##########KMEANS######################################
elif page == 'Agrupamiento K-Means':
    st.subheader('Clustering')
    groups = st.number_input(label = 'Cantidad de grupos (2 - 10):', min_value = 2, max_value = 10, step = 1)

    kmeans_fig=evaluate_clustering2('TITULO', groups)
    st.pyplot(kmeans_fig)
    #silhouette_fig, silhouette, davies, calinski = evaluate_clustering2('TITULO', groups)
    #voronoi_fig, _, _, _ = represent_kmeans('', groups)



    #st.subheader('Gráfico de Silhouette')
    #st.pyplot(silhouette_fig)
    #st.write('Silhouette medio: ' + str(silhouette))

    #st.subheader('Diagrama de Voronoi')
    #st.pyplot(voronoi_fig)

##########SILHOUETTE######################################
elif page == 'Análisis de Silhouette':
    st.subheader("Rango del análisis")

    clusters = None

    with st.form("values"):
        col1, col2 = st.columns([2, 2])

        with col1:
            min_groups = st.number_input(label = 'Mínima cantidad de grupos (2 - 10):', min_value = 2, max_value = 10, step = 1)
            step_groups = st.number_input(label = 'Paso (1 - 3):', min_value = 1, max_value = 3, step = 1)

        with col2:
            max_groups = st.number_input(label = 'Máxima cantidad de grupos (2 - 10):', min_value = 2, max_value = 10, step = 1)

        submitted = st.form_submit_button("Iniciar análisis")

        if submitted:
            clusters = list(range(min_groups, max_groups + 1, step_groups))

    if clusters != None:
        with st.spinner('Ejecutando análisis...'):
            st.subheader('Gráficos de Silhouette')

            for i in range(len(clusters)):
                fig, _, = silhouette(df, clusters[i])

                st.pyplot(fig)
           
############################CLASIFICACION#####################
elif page == 'Modelos de clasificación':
    model = st.selectbox(label = 'Seleccione un modelo', options = ('KNN', 'Logistic Regression', 'SVM', 'Random Forest'), index = 0)

    show_scores = False

    num_trn=13000 #number of rows for training (66,8%)
    num_tes=6457 #number of rows for testing (33,2%)
    X_train,X_test,Y_train,Y_test=df[:num_trn],df[num_trn:num_trn+num_tes],isapt[:num_trn],isapt[num_trn:num_trn+num_tes]
    Y_train_APT=(Y_train == 1)
    Y_test_APT=(Y_test == 1)

###KNN###
    if model == 'KNN':
        knn_file="../results/knn_optimized_parameters.bin"

        #Read las results
        if path.exists(knn_file):
          with open(knn_file) as f:
            lines = f.readlines()
            last_estimators=lines[0].rstrip()    #n
            st.write("En la ultima sintonizacion, Number of estimators fue de "+str(last_estimators))
            f.close()

        if st.button('Sintonizar número de vecinos óptimo'):
               with st.spinner('Sintonizando hiperparámetros de KNN...'):
                   vecinos = range(2, 10, 1)
                   best_n = tune_knn(vecinos, X_train, Y_train_APT, X_test, Y_test_APT)

               st.subheader("Hiperparámetros sintonizados")
               st.write('Número de vecinos óptimo: ' + str(best_n))

        with st.form("hyperparameters"):
            vec = st.number_input(label = 'Número de vecinos (2 - 9):', min_value = 2, max_value = 9, step = 1)

            submitted = st.form_submit_button("Entrenar modelo")
                
            if submitted:
                with st.spinner('Entrenando clasificador KNN...'):
                    model = neighbors.KNeighborsClassifier(n_neighbors = vec)
                    show_scores = True

                    [KNN_kappa, KNN_Acc, KNN_r, KNN_pr, KNN_f1, KNN_cm] = evaluate_model_instance(model, True, X_train, Y_train, X_test, Y_test_APT)

                    ###
                    st.write("Accuracy: ", KNN_Acc)
                    st.write("Precision: ", KNN_pr)
                    st.write("Recall: ", KNN_r)
                    st.write("Kappa: ", KNN_kappa)
                    st.write("F1: ", KNN_f1)
                    st.write("Confusion Matrix: ", KNN_cm)
                    values = [KNN_kappa, KNN_Acc, KNN_r, KNN_pr, KNN_f1]

                    #[kappa, accuracy, recall, precision, f1] = evaluate_model_instance(model, True, X_train, Y_train, X_test, Y_test_APT)
                    #st.subheader("Modelo Entrenado")
                    #st.write(accuracy)
        
## LR ###############################################        
    elif model == 'Logistic Regression':
        lr_file="../results/lr_optimized_parameters.bin"

        #Read las results
        if path.exists(lr_file):
          with open(lr_file) as f:
            lines = f.readlines()
            last_regularization_strength=lines[0].rstrip()    #n
            last_penalty=lines[1].rstrip()   #f 
            last_solver=lines[2].rstrip()   #f 
            st.write("En la ultima sintonizacion, los resultados fueron:")
            st.write("Regularization strength="+str(last_regularization_strength)+", Penalty="+str(last_penalty)+", Solver="+str(last_solver))
            f.close()

        if st.button('Sintonizar hiperparámetros'):
            with st.spinner('Sintonizando hiperparámetros de Logistic Regression (Esta tarea puede tardar más de 10 minutos)...'):
                Crange=range(5,20,1)
                penaltyOptions=['l1', 'l2', 'elasticnet', 'none']
                solver=['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']
                penaltyOptions={
                        "newton-cg": ['l2', 'none'],
                        "lbfgs": ['l2', 'none'],
                        "liblinear": ['l1', 'l2'],
                        "sag": ['l2', 'none'],
                        "saga": ['elasticnet', 'l1', 'l2', 'none'] }
                best_p, best_c, best_s = tune_logistic_regression(solver, Crange, penaltyOptions, X_train, Y_train_APT, X_test, Y_test_APT)

            st.subheader("Hiperparámetros òptimos:")

            st.write('Regularization strength: ' + str(best_c))
            st.write('Penalty: ' + str(best_p/10))
            st.write('Solver: ' + str(best_s))

        with st.form("hyperparameters"):

            C_ = st.number_input(label = 'Regularization strength (0.5 - 2):', min_value = 0.5, max_value = 2.0, step = 0.1)
            solver_ = st.selectbox(label = 'Solver:', options = ('newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'))
            penalty_ = st.selectbox(label = 'Penalty:', options = ('l1', 'l2', 'elasticnet', 'none'))
            submitted = st.form_submit_button("Entrenar modelo")
                
            if submitted:
                with st.spinner('Entrenando logistic regression...'):
                    show_scores = True
                    model = LogisticRegression(penalty=penalty_, C=C_)
                    [LR_kappa, LR_Acc, LR_r, LR_pr, LR_f1, LR_cm] = evaluate_model_instance(model, True, X_train, Y_train_APT, X_test, Y_test_APT)
                    ###
                    st.write("Accuracy: ", LR_Acc)
                    st.write("Precision: ", LR_pr)
                    st.write("Recall: ", LR_r)
                    st.write("Kappa: ", LR_kappa)
                    st.write("F1: ", LR_f1)
                    st.write("Confusion Matrix: ", LR_cm)
                    values = [LR_kappa, LR_Acc, LR_r, LR_pr, LR_f1]

## SVM ###############################################        
    elif model == 'SVM':
        svm_file="../results/svm_optimized_parameters.bin"

        #Read las results
        if path.exists(svm_file):
          with open(svm_file) as f:
            lines = f.readlines()
            last_C=lines[0].rstrip()    #n
            last_gamma=lines[1].rstrip()   #f 
            st.write("En la ultima sintonizacion, los resultados fueron:")
            st.write("C="+str(last_C)+", Gamma="+str(last_gamma))
            f.close()

        if st.button('Sintonizar hiperparámetros'):
            Crange = range(1,120,24)
            gammaRange = range(1,120,24)

            with st.spinner('Sintonizando hiperparámetros de SVM (Esta tarea puede tardar 10 minutos)...'):
                best_c, best_g = tune_svc(Crange, gammaRange, X_train, Y_train_APT, X_test, Y_test_APT)

                st.subheader("Hiperparámetros sintonizados")

                st.write('Parámetro de regularización (C): ' + str(best_c/100))
                st.write('Gamma: ' + str(best_g))
           
        with st.form("hyperparameters"):
            C_ = st.number_input(label = 'Parámetro de regularización C (0.01 - 500):', min_value=0.01, max_value=500.0, value = 1.0, step = 0.05)
            gamma_ = st.number_input(label = 'Gamma (0.125 - 256):', min_value = 0.125, max_value=256.0, value = 4.0, step = 1.0)

            submitted = st.form_submit_button("Entrenar modelo")
                
            if submitted:
                with st.spinner('Entrenando clasificador SVM...'):
                    show_scores = True
                    model=svm.SVC(kernel='linear', probability=True, C=C_/100, gamma=gamma_)
                    [SVM_kappa, SVM_Acc, SVM_r, SVM_pr, SVM_f1, SVM_cm] = evaluate_model_instance(model,True, X_train, Y_train_APT, X_test, Y_test_APT)

                    ###
                    st.write("Accuracy: ", SVM_Acc)
                    st.write("Precision: ", SVM_pr)
                    st.write("Recall: ", SVM_r)
                    st.write("Kappa: ", SVM_kappa)
                    st.write("F1: ", SVM_f1)
                    st.write("Confusion Matrix: ", SVM_cm)
                    values = [SVM_kappa, SVM_Acc, SVM_r, SVM_pr, SVM_f1]

## RF  ###############################################        
    elif model == 'Random Forest':
        rf_file="../results/rf_optimized_parameters.bin"

        #Read las results
        if path.exists(rf_file):
          with open(rf_file) as f:
            lines = f.readlines()
            last_n_estimator=lines[0].rstrip()    #n
            last_max_features=lines[1].rstrip()   #f 
            last_max_depths=lines[2].rstrip()   #d 
            last_sample_split=lines[3].rstrip()  #s  
            last_criterion=lines[4].rstrip()   #c 
            st.write("En la ultima sintonizacion, los resultados fueron:")
            st.write("n_estimators="+str(last_n_estimator)+", max_features="+str(last_max_features)+", max_depth="+str(last_max_depths)+", min_samples_split="+str(last_sample_split)+", criterion="+str(last_criterion))
            f.close()

        samples_splits = range(2,20,2)
        max_features = ['sqrt', 'log2']
        max_depths = range(10,100,10)
        criterions = ['gini', 'entropy']
        n_estimators = range(10,150,10)
               
        if st.button('Sintonizar hiperparámetros'):
            with st.spinner('Sintonizando hiperparámetros de Random Forest...'):
                best_n, best_f, best_d, best_s, best_c = tune_random_forest(n_estimators, max_features, max_depths, samples_splits, criterions, X_train, Y_train_APT, X_test, Y_test_APT)

                if best_f == 'sqrt':
                    best_f = 'raíz cuadrada'
                elif best_f == 'log2':
                    best_f = 'logaritmo binario'

                if best_c == 'entropy':
                    best_c = 'entropía'
                else:
                    best_c = 'índice de Gini'

                st.subheader("Hiperparámetros sintonizados")

                col1, col2, col3 = st.columns(3)
            
                with col1:
                    st.write('Número mínimo de muestras para dividir: ' + str(best_s))
                    st.write('Máximo de características por nodo:' + best_f)

                with col2:
                    st.write('Máxima profundidad: ' + str(best_d))
                    st.write('Criterio: ' + best_c)
                    
                with col3:
                    st.write('Número de árboles: ' + str(best_n))


        with st.form("hyperparameters"):
            col1, col2, col3 = st.columns(3)

            with col1:
                min_samples_split_ = st.number_input(label = 'Muestras para dividir (2 - 20):', min_value = 2, max_value = 20, step = 2)
                max_features_ = st.selectbox(label = 'Máximo de características por nodo:', options = ('raíz cuadrada', 'logaritmo binario'))
                
            with col2:
                max_depth_ = st.number_input(label = 'Máxima profundidad (0 - 1000):', min_value = 0, max_value = 1000, step = 10)
                criterion = st.selectbox(label = 'Criterio:', options = ('índice de Gini', 'entropía'))
                
            with col3:
                number_estimators = st.number_input(label = 'Número de árboles (10 - 3000):', min_value = 10, max_value = 3000, value = 100, step = 10)
                
            submitted = st.form_submit_button("Entrenar modelo")

            if submitted:
                with st.spinner('Entrenando clasificador Random Forest...'):
                    mf = ''
                    c = ''

                    if max_features == 'raíz cuadrada':
                        mf = 'sqrt'
                    else:
                        mf = 'log2'

                    if criterion == 'índice de Gini':
                        c = 'gini'
                    else:
                        c = 'entropy'

                    if max_depth_ == 0:
                        max_depth_ = None

                    show_scores = True
                    model= RandomForestClassifier(random_state=42,n_estimators=number_estimators, criterion=c, max_depth=max_depth_, min_samples_split=min_samples_split_, max_features=max_features_ )
                    [RF_kappa, RF_Acc, RF_r, RF_pr, RF_f1, RF_cm] = evaluate_model_instance(model,True, X_train, Y_train_APT, X_test, Y_test_APT)
                    values = [kappa, accuracy, recall, precision, f1]
    

    if show_scores:
        st.subheader("Medidas de calidad")

        fields = ['Kappa', 'Accuracy', 'Recall', 'Precision', 'F1']

        st.pyplot(quality_scores(fields, values))

###########################################################################################
elif page == 'Comparativa':
   
    if (data=='Primer dataset'):
        num_trn=13000 #number of rows for training (66,8%)
        num_tes=6457 #number of rows for testing (33,2%)
        X_train,X_test,Y_train,Y_test=df[:num_trn],df[num_trn:num_trn+num_tes],isapt[:num_trn],isapt[num_trn:num_trn+num_tes]
    elif (data=='Segundo dataset'):
        num_trn=3250 #number of rows for training (66,8%)
        num_tes=1625 #number of rows for testing (33,2%)
        X_train,X_test,Y_train,Y_test=segundo_df[:num_trn],segundo_df[num_trn:num_trn+num_tes],isapt[:num_trn],isapt[num_trn:num_trn+num_tes]

    Y_train_APT=(Y_train == 1)
    Y_test_APT=(Y_test == 1)

    score = st.selectbox(label = 'Medida de calidad:', options = ('Kappa', 'Accuracy', 'Recall', 'Precision', 'F1'), index = 0)

    svm_file="../results/svm_optimized_parameters.bin"
    knn_file="../results/knn_optimized_parameters.bin"
    lr_file="../results/lr_optimized_parameters.bin"
    rf_file="../results/rf_optimized_parameters.bin"

    kappa = [None] * 5
    accuracy = [None] * 5
    recall = [None] * 5
    precision = [None] * 5
    f1 = [None] * 5


    if path.exists(rf_file) and path.exists(lr_file) and path.exists(svm_file) and path.exists(knn_file):
      with st.spinner('Entrenando modelos óptimos...'):
        with open(rf_file) as f:
            lines = f.readlines()
            last_n_estimator=lines[0].rstrip()    #n
            last_max_features=lines[1].rstrip()   #f 
            last_max_depths=lines[2].rstrip()   #d 
            last_sample_split=lines[3].rstrip()  #s  
            last_criterion=lines[4].rstrip()   #c 
            f.close()

        with open(lr_file) as g:
            lines = g.readlines()
            last_regularization_strength=lines[0].rstrip()    #n
            last_penalty=lines[1].rstrip()   #f 
            last_solver=lines[2].rstrip()   #f 
            g.close()

        with open(svm_file) as h:
            lines = h.readlines()
            last_C=lines[0].rstrip()    #n
            last_gamma=lines[1].rstrip()   #f 
            h.close()

        with open(knn_file) as i:
            lines = i.readlines()
            last_neighbors=lines[0].rstrip()    #n
            i.close()

        SVM_model=svm.SVC(kernel='linear', probability=True, C=float(last_C)/100.0, gamma='scale')
        LR_model = LogisticRegression(penalty=last_penalty)    ###?????
        print (last_neighbors)
        KNN_model = neighbors.KNeighborsClassifier(n_neighbors = 2)
        RF_model= RandomForestClassifier(random_state=42,n_estimators=int(last_n_estimator), criterion=last_criterion, max_depth=int(last_max_depths), min_samples_split=int(last_sample_split), max_features=last_max_features )

        [kappa[0], accuracy[0], recall[0], precision[0], f1[0],_] = evaluate_model_instance(KNN_model,True, X_train, Y_train_APT, X_test, Y_test_APT)
        [kappa[1], accuracy[1], recall[1], precision[1], f1[1],_] = evaluate_model_instance(LR_model,True, X_train, Y_train_APT, X_test, Y_test_APT)
        [kappa[2], accuracy[2], recall[2], precision[2], f1[2],_] = evaluate_model_instance(SVM_model,True, X_train, Y_train_APT, X_test, Y_test_APT)
        [kappa[3], accuracy[3], recall[3], precision[3], f1[3],_] = evaluate_model_instance(RF_model,True, X_train, Y_train_APT, X_test, Y_test_APT)
        [kappa[4], accuracy[4], recall[4], precision[4], f1[4],_] = [0.0,0.0,0.0,0.0,0.0,0.0]

        fields = ['KNN', 'Logistic Regression', 'SVM', 'Random Forest']

#        st.pyplot(quality_scores(fields, kappa))
#        st.pyplot(quality_scores(fields, accuracy))
#        st.pyplot(quality_scores(fields, recall))
#        st.pyplot(quality_scores(fields, precision))
#        st.pyplot(quality_scores(fields, f1))
    else:
        st.error("No se dispone de informacion de todos los algoritmos")

##########
# ROC Curve
#
# Esta grafica nos permite decidir cual de los clasificadores son mejores
###########
#### AUC #######################################################################################
#elif page == 'AUC':

    # ROC AUC (Area under Curve) Score

## SVM ##########
    y_probabilities_svm=cross_val_predict(SVM_model, X_train, Y_train_APT, cv=10)
    y_scores_svm = SVM_model.decision_function(X_test)
    fpr_svm, tpr_svm, threshold_svm=roc_curve(Y_test_APT,y_scores_svm)
    SVM_roc_auc = auc(fpr_svm, tpr_svm)

## KNN ##########
    y_probabilities_knn=cross_val_predict(KNN_model, X_test, Y_test_APT, cv=10, method="predict_proba")
    y_scores_knn=y_probabilities_knn[:,1]
    fpr_knn, tpr_knn, threshold_knn=roc_curve(Y_test_APT,y_scores_knn)
    KNN_roc_auc = auc(fpr_knn, tpr_knn)

#######LR ########
    lr_predict_probabilities = LR_model.predict_proba(X_test)[:,1]

    fpr_lr, tpr_lr, threshold_lr = roc_curve(Y_test_APT, lr_predict_probabilities)
    LOG_roc_auc = auc(fpr_lr, tpr_lr)

#######RANDOM FOREST########
    y_probabilities_forest=cross_val_predict(RF_model, X_test, Y_test_APT, cv=10, method="predict_proba")
    y_scores_forest=y_probabilities_forest[:,1]
    RAN_roc_auc=roc_auc_score(Y_test_APT, y_scores_forest)

    y_scores_forest=y_probabilities_forest[:,1]
    fpr_forest, tpr_forest, threshold_forest=roc_curve(Y_test_APT,y_scores_forest)
    roc_auc = auc(fpr_forest, tpr_forest)

####### PLOT ########
    fig = plt.figure(figsize=(15, 15))

    plt.plot(fpr_forest, tpr_forest, linewidth=2, label='Roc Curve de Random Forest Classifier (area= %0.4f)' % RAN_roc_auc)

    plt.plot(fpr_lr, tpr_lr, linewidth=2, label='Roc Curve de Logistic Regression (area= %0.4f)' % LOG_roc_auc)
    plt.plot(fpr_svm, tpr_svm, linewidth=2, label='Roc Curve de SVM (area= %0.4f)' % SVM_roc_auc)
    plt.plot(fpr_knn, tpr_knn, linewidth=2, label='Roc Curve de KNN(area = %0.4f)' % KNN_roc_auc)

    fig.tight_layout()
    plt.plot([0,1],[0,1],"k--")
    plt.axis([0,1,0,1])
    plt.xlabel("False Positive rate")
    plt.ylabel("True Positive rate")
    plt.legend()

